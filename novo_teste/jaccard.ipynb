{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881779b2",
   "metadata": {},
   "source": [
    "Come√ßaremos o projeto aplicando o √≠ndice de Jaccard e o filtro TPEI nos dados iniciais.\n",
    "O filtro TPEI excluir√° grande parte dos dados desnecess√°rios pois a equival√™ncia s√≥ ser√° v√°lida para uma quantidade de TPEI igual ou maior. O √≠ndice de Jaccard analisar√° os dados a partir de um grafo direcionado baseado nas recomenda√ß√µes de uma disciplina. Sendo assim, √© necess√°rio criar esse grafo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4eecd",
   "metadata": {},
   "source": [
    "Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7eb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import unicodedata\n",
    "import requests\n",
    "from io import StringIO\n",
    "from node2vec import Node2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba20700",
   "metadata": {},
   "source": [
    "Preparar os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75caac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Normaliza uma string removendo acentos e espa√ßos extras\n",
    "def normalize_str(s: str) -> str:\n",
    "    return (\n",
    "        unicodedata.normalize('NFKD', s)\n",
    "        .encode('ASCII', 'ignore')\n",
    "        .decode('utf-8')\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "# ‚úÖ Vers√£o espec√≠fica para nomes de disciplinas (j√° normaliza e coloca lowercase)\n",
    "def normalizar_nome(nome: str) -> str:\n",
    "    return normalize_str(nome).lower()\n",
    "\n",
    "# ‚úÖ Baixa o cat√°logo da UFABC direto do GitHub\n",
    "def carregar_catalogo():\n",
    "    url = \"https://raw.githubusercontent.com/angeloodr/disciplinas-ufabc/main/catalogo_disciplinas_graduacao_2024_2025.tsv\"\n",
    "    print(\"üîÑ Baixando cat√°logo de disciplinas...\")\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    df = pd.read_csv(StringIO(resp.text), sep='\\t')\n",
    "\n",
    "    # Normaliza os nomes das colunas\n",
    "    df.columns = [\n",
    "        normalize_str(col).upper().replace(' ', '_')\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    print(\"‚úÖ Download bem-sucedido!\")\n",
    "    print(\"üìù Colunas dispon√≠veis:\", df.columns.tolist())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a4053",
   "metadata": {},
   "source": [
    "Construir o grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9566a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Constr√≥i grafo com arestas de pr√©-requisito com base na coluna RECOMENDACAO\n",
    "def construir_grafo(df: pd.DataFrame) -> nx.DiGraph:\n",
    "    print(\"üìå Construindo grafo de pr√©-requisitos...\")\n",
    "    G = nx.DiGraph()\n",
    "    total_arestas = 0\n",
    "\n",
    "    # Cria um dicion√°rio de nome normalizado ‚Üí sigla\n",
    "    mapping = {\n",
    "        normalizar_nome(row['DISCIPLINA']): row['SIGLA']\n",
    "        for _, row in df.iterrows()\n",
    "        if pd.notna(row['SIGLA'])\n",
    "    }\n",
    "\n",
    "    # Adiciona todos os n√≥s com metadados\n",
    "    for _, row in df.iterrows():\n",
    "        sigla = row['SIGLA']\n",
    "        nome = row['DISCIPLINA']\n",
    "        if pd.isna(sigla): \n",
    "            continue\n",
    "        G.add_node(sigla, nome=nome)\n",
    "\n",
    "    # Adiciona as arestas baseadas nas recomenda√ß√µes\n",
    "    for _, row in df.iterrows():\n",
    "        curso = row['SIGLA']\n",
    "        if pd.isna(curso):\n",
    "            continue\n",
    "\n",
    "        recs = row.get('RECOMENDACAO', '')\n",
    "        if pd.isna(recs) or not isinstance(recs, str):\n",
    "            continue\n",
    "\n",
    "        for rec in recs.split(';'):\n",
    "            rec = rec.strip()\n",
    "            if not rec:\n",
    "                continue\n",
    "            rec_norm = normalizar_nome(rec)\n",
    "            if rec_norm in mapping:\n",
    "                prereq = mapping[rec_norm]\n",
    "                if not G.has_edge(prereq, curso):\n",
    "                    G.add_edge(prereq, curso, tipo='pre_requisito')\n",
    "                    total_arestas += 1\n",
    "\n",
    "    print(f\"‚úÖ Grafo criado com {G.number_of_nodes()} n√≥s e {total_arestas} arestas.\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a415e6",
   "metadata": {},
   "source": [
    "Configurar o grafo para rede direcionada e criar os embenddings estruturais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Remove ciclos do grafo para que seja um DAG (necess√°rio para c√°lculo de profundidade)\n",
    "def remover_ciclos(G: nx.DiGraph) -> nx.DiGraph:\n",
    "    print(\"üîÅ Removendo ciclos (modo eficiente)...\")\n",
    "    G_copy = G.copy()\n",
    "    removidos = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ciclo = nx.find_cycle(G_copy, orientation='original')\n",
    "            if ciclo:\n",
    "                # Remove a primeira aresta do ciclo\n",
    "                u, v, _ = ciclo[0]\n",
    "                G_copy.remove_edge(u, v)\n",
    "                removidos += 1\n",
    "    except nx.NetworkXNoCycle:\n",
    "        pass  # N√£o h√° mais ciclos\n",
    "\n",
    "    print(f\"‚úÖ Ciclos removidos: {removidos}\")\n",
    "    return G_copy\n",
    "\n",
    "# ‚úÖ Gera embeddings estruturais com Node2Vec\n",
    "def gerar_embeddings_node2vec(G: nx.DiGraph, dimensions=64, walk_length=10, num_walks=50, workers=1):\n",
    "    print(\"üîÑ Gerando embeddings estruturais (Node2Vec)...\")\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers)\n",
    "    model = node2vec.fit(window=5, min_count=1, batch_words=4)\n",
    "    embeddings = {node: model.wv[node] for node in G.nodes()}\n",
    "    print(\"‚úÖ Embeddings gerados!\")\n",
    "    return embeddings\n",
    "\n",
    "# ‚úÖ Calcula profundidade de cada n√≥ no grafo (camada curricular)\n",
    "def calcular_profundidade(G: nx.DiGraph):\n",
    "    print(\"üìè Calculando profundidade no grafo (DAG)...\")\n",
    "    profundidades = {}\n",
    "    for node in nx.topological_sort(G):\n",
    "        preds = list(G.predecessors(node))\n",
    "        if not preds:\n",
    "            profundidades[node] = 0\n",
    "        else:\n",
    "            profundidades[node] = max(profundidades[p] for p in preds) + 1\n",
    "    print(\"‚úÖ Profundidade calculada!\")\n",
    "    return profundidades\n",
    "\n",
    "# ‚úÖ Salva os embeddings em CSV\n",
    "def salvar_embeddings_csv(embeddings, caminho=\"embeddings_node2vec.csv\"):\n",
    "    df = pd.DataFrame.from_dict(embeddings, orient='index')\n",
    "    df.index.name = 'DISCIPLINA'\n",
    "    df.to_csv(caminho)\n",
    "    print(f\"üíæ Embeddings salvos em: {caminho}\")\n",
    "\n",
    "# ‚úÖ Fluxo principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Etapa 1 - Carregar cat√°logo\n",
    "    df = carregar_catalogo()\n",
    "\n",
    "    # Etapa 2 - Construir grafo\n",
    "    grafo = construir_grafo(df)\n",
    "    nx.write_graphml(grafo, \"grafo_pre_requisitos.graphml\")\n",
    "    print(\"üìÅ Arquivo salvo: grafo_pre_requisitos.graphml\")\n",
    "\n",
    "    # Etapa 3 - Gerar embeddings estruturais\n",
    "    embeddings = gerar_embeddings_node2vec(grafo)\n",
    "    salvar_embeddings_csv(embeddings)\n",
    "\n",
    "    # Etapa 4 - Remover ciclos e calcular profundidade\n",
    "    grafo_sem_ciclos = remover_ciclos(grafo)\n",
    "    profundidades = calcular_profundidade(grafo_sem_ciclos)\n",
    "\n",
    "    # Etapa 5 - Salvar profundidades em TXT\n",
    "    with open(\"profundidade_nos.txt\", \"w\") as f:\n",
    "        for node, prof in profundidades.items():\n",
    "            f.write(f\"{node}: {prof}\\n\")\n",
    "    print(\"üìÅ Profundidades salvas em: profundidade_nos.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b752ef",
   "metadata": {},
   "source": [
    "Com o grafo pronto e preparado para o uso ao executar os devidos carregamentos, come√ßaremos a aplica√ß√£o do Jaccard e outras similaridades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b6057ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¢ Jaccard entre dois conjuntos\n",
    "def jaccard_similarity(set1, set2):\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "# üîÅ Aplica Jaccard entre predecessores ou sucessores\n",
    "def similaridade_jaccard(G, a, b, tipo='predecessor'):\n",
    "    try:\n",
    "        if tipo == 'predecessor':\n",
    "            set_a = set(G.predecessors(a))\n",
    "            set_b = set(G.predecessors(b))\n",
    "        else:\n",
    "            set_a = set(G.successors(a))\n",
    "            set_b = set(G.successors(b))\n",
    "        return jaccard_similarity(set_a, set_b)\n",
    "    except nx.NetworkXError:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754709ca",
   "metadata": {},
   "source": [
    "Aplicar similaridade cosseno e de profundidade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94bce001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìè Similaridade de profundidade\n",
    "def similaridade_profundidade(prof, a, b):\n",
    "    if a not in prof or b not in prof:\n",
    "        return 0.0\n",
    "    max_p = max(prof.values())\n",
    "    if max_p == 0:\n",
    "        return 1.0\n",
    "    return 1.0 - abs(prof[a] - prof[b]) / max_p\n",
    "\n",
    "# üß¨ Similaridade por Node2Vec (cosseno)\n",
    "def similaridade_node2vec(embeddings, a, b):\n",
    "    if a not in embeddings.index or b not in embeddings.index:\n",
    "        return 0.0\n",
    "    vec_a = embeddings.loc[a].values.reshape(1, -1)\n",
    "    vec_b = embeddings.loc[b].values.reshape(1, -1)\n",
    "    return cosine_similarity(vec_a, vec_b)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb1c65",
   "metadata": {},
   "source": [
    "Combinar as diferentes similaridades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e055661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Combina m√∫ltiplas similaridades em score final\n",
    "def similaridade_combinada(G, embeddings, profundidades, a, b, pesos=None):\n",
    "    if pesos is None:\n",
    "        pesos = {\n",
    "            'jaccard_pred': 1.0,\n",
    "            'jaccard_succ': 1.0,\n",
    "            'profundidade': 1.0,\n",
    "            'node2vec': 1.0,\n",
    "        }\n",
    "\n",
    "    sim_jaccard_pred = similaridade_jaccard(G, a, b, tipo='predecessor')\n",
    "    sim_jaccard_succ = similaridade_jaccard(G, a, b, tipo='successor')\n",
    "    sim_profundidade = similaridade_profundidade(profundidades, a, b)\n",
    "    sim_node2vec = similaridade_node2vec(embeddings, a, b)\n",
    "\n",
    "    total_peso = sum(pesos.values())\n",
    "    score = (\n",
    "        pesos['jaccard_pred'] * sim_jaccard_pred +\n",
    "        pesos['jaccard_succ'] * sim_jaccard_succ +\n",
    "        pesos['profundidade'] * sim_profundidade +\n",
    "        pesos['node2vec'] * sim_node2vec\n",
    "    ) / total_peso\n",
    "\n",
    "    return {\n",
    "        \"score_combinado\": score,\n",
    "        \"jaccard_pred\": sim_jaccard_pred,\n",
    "        \"jaccard_succ\": sim_jaccard_succ,\n",
    "        \"profundidade\": sim_profundidade,\n",
    "        \"node2vec\": sim_node2vec\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc82ee6",
   "metadata": {},
   "source": [
    "Filtro TPEI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df5e702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Filtro TPEI exato: compara atributos T, P, E, I\n",
    "def filtro_tpei_exato(G, a, b):\n",
    "    for campo in ['T', 'P', 'E', 'I']:\n",
    "        if G.nodes[a].get(campo) != G.nodes[b].get(campo):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5df355",
   "metadata": {},
   "source": [
    "Com isso, temos nossos dados preparados para a pr√≥xima etapa. O intuito √© diminuir os dados de forma eficiente a cada etapa para reduzir o custo computacional final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
