{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343744a0",
   "metadata": {},
   "source": [
    "# Sistema de DetecÃ§Ã£o de EquivalÃªncia de Disciplinas\n",
    "\n",
    "# Notebook de verificaÃ§Ã£o da similaridade entre ementas de disciplinas\n",
    "\n",
    "## IntroduÃ§Ã£o:\n",
    "O estudante da UFABC passa muito tempo jÃ¡ em sua graduaÃ§Ã£o para terminar as disciplinas do seu BI e do pÃ³s-BI e isso afeta principalmente os alunos de cursos mais concorridos como Ã© o de computaÃ§Ã£o, em que as disciplinas podem ter mais de 150% de requisiÃ§Ã£o. Tendo em vista isso, na UFABC temos dois processos jÃ¡ estruturados que Ã© o processo de covalidaÃ§Ã£o e de equivalÃªncia, normatizados nas resoluÃ§Ãµes ConsEPE nÂº 157/2013 e CG NÂº 023/2019 respectivamente. CovalidaÃ§Ã£o Ã© um processo interno da UFABC que Ã© basicamente para a transiÃ§Ã£o de projetos pedagÃ³gicos, de forma que o(a) estudante consegue integralizar o curso em um PPC antigo com disciplinas novas e a equivalÃªncia Ã© um processo que uma disciplina de fora pode ter alguma similaridade de uma disciplina da ufabc e do curso que vocÃª quer se formar. Assim estÃ¡ presente na ResoluÃ§Ã£o CG NÂº 023/2019 o seguinte:\n",
    "\n",
    ">Art. 4Âº Consistem em requisitos para a dispensa por equivalÃªncia, para disciplinas\n",
    "cursadas no Brasil:\n",
    "\n",
    ">> I. a carga horÃ¡ria total da disciplina cursada deve ser igual ou maior Ã  carga horÃ¡ria da que se pede equivalÃªncia;\n",
    "\n",
    ">> II. o conteÃºdo da disciplina cursada deve ser compatÃ­vel e correspondente a, no mÃ­nimo, 75% (setenta e cinco por cento) do conteÃºdo daquela de que se pede equivalÃªncia, considerando-se teoria e prÃ¡tica, quando for o caso. \n",
    "\n",
    ">>>ParÃ¡grafo Ãºnico: Excepcionalmente, e mediante justificativa, a coordenaÃ§Ã£o de curso pode autorizar equivalÃªncias que cumpram parcialmente estes requisito\n",
    "\n",
    "Utilizando essa base das normativas e os catÃ¡logos de disciplinas da UFABC, objetivamos gerar um sistema de prÃ©-avaliaÃ§Ã£o de disciplinas com alta chance de convalidaÃ§Ã£o, reduzindo o espaÃ§o de busca dos tÃ©cnicos responsaveis pela aprovaÃ§Ã£o de pares de disciplinas com convalidaÃ§Ã£o e garantindo a inclusÃ£o de todas as disciplinas na anÃ¡lise. Os benefÃ­cio desta proposta sÃ£o a economia de recursos, maior integralizaÃ§Ã£o de diferentes PPCs e promove efetivamente a interdisciplinaridade, fundamento da UFAB, uma vez que os cursos poderiam ofertar a mesma disciplina com diferentes enfoques no mesmo quadrimestre, melhorando a qualidade do ensino. Os recursos poupados sÃ£o tanto na carga de trabalho dos tÃ©cnicos quanto recursos computacionais (realizada manualmente, ess tarefa tem um complexidade O(n^2)), requerindo anÃ¡lise manual apenas das disciplinas que se encaixam nos critÃ©rios das resoluÃ§Ãµes ConsEPE nÂº 157/2013 e CG NÂº 023/2019 e com alta probabilidade de validaÃ§Ã£o. \n",
    "\n",
    "Dessa forma propomos uma anÃ¡lise de similaridade semantica e relacional entre as ementas, os prÃ©-requisitos das disciplinas da UFABC e dos valores de TPEI, para fim de prÃ©-avaliaÃ§Ã£o de equivalÃªncias entre as diciplinas da universidade cumprindo com o Art. 4Â° da resoluÃ§Ã£o CG NÂº 023/2019. Assim. em disciplinas que verificarmos que existe uma similaridade equivalente a de pares de disciplinas atualmente validadas (similaridade maior ou igual a 75% e que cumpre a quantidade de creditos da outra disciplina), poderemos gerar listas de pares para aprovaÃ§Ã£o manual em ordem de similaridde. O benefÃ­cio proporcionado pela maior integraÃ§Ã£o do sistema Ã© difÃ­cil de mensurar mas, alÃ©m de afetar todes es envolvides no processo discente, o alinhamento dessa soluÃ§Ã£o com os sistemas vigentes da universidade amplificam seu impacto e condiÃ§Ãµes em que ele se manifesta.\n",
    "\n",
    "## Objetivos\n",
    "1. Fazer uma proposta de equivalÃªncia interna de disciplinas a partir da anÃ¡lise de similaridade semÃ¢ntica entre as ementas das disciplinas da UFABC da GraduaÃ§Ã£o e da quantidade de TPEI que elas tem\n",
    "2. Reduzir o espaÃ§o de busca de O(nÂ²) para aproximadamente O(n*log(n)) a partir da aplicaÃ§Ã£o de filtros de TPEI para quantidade de crÃ©ditos das disciplinas, de forma sÃ³ comparar as diciplinas que tem a quantidade de crÃ©ditos iguais\n",
    "\n",
    "## MÃ©todos\n",
    "1. Embedding semÃ¢ntico do conteÃºdo dos Objetivos e Ementas das disciplinas utilizando BERTimbau\n",
    "2. CÃ¡lculo de similaridade cosseno para similaridade entre os textos \n",
    "3. Kmeans nos embeddings para determinar agrupamentos de disciplinas ajudando no processo de equivalÃªncia\n",
    "4. Grafo relacional dos PrÃ©-Requisitos entre disciplinas\n",
    "5. CÃ¡lculo de similaridade utiliando distÃ¢ncias de jaccard\n",
    "6. OtimizaÃ§Ã£o dos hiperparÃ¢metros dos filtros de reduÃ§Ã£o do espaÃ§o de busca utilizando modelos de Ã¡rvores aleatÃ³rias \n",
    "\n",
    "## 1. Imports e ConfiguraÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3772ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza imports necessÃ¡rios\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import unicodedata\n",
    "import requests\n",
    "from io import StringIO\n",
    "from node2vec import Node2Vec\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85da40",
   "metadata": {},
   "source": [
    "## 2. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f4b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixa o catÃ¡logo da UFABC direto do RepositÃ³rio no GitHub\n",
    "def carregar_catalogo():\n",
    "    url = \"https://raw.githubusercontent.com/angeloodr/disciplinas-ufabc/main/catalogo_disciplinas_graduacao_2024_2025.tsv\"\n",
    "    print(\"ðŸ”„ Baixando catÃ¡logo de disciplinas...\")\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    df = pd.read_csv(StringIO(resp.text), sep='\\t')\n",
    "\n",
    "    # Normaliza os nomes das colunas\n",
    "    df.columns = [\n",
    "        normalize_str(col).upper().replace(' ', '_')\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    print(\"âœ… Download bem-sucedido!\")\n",
    "    print(\"ðŸ“ Colunas disponÃ­veis:\", df.columns.tolist())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7a644",
   "metadata": {},
   "source": [
    "## 3. PreparaÃ§Ã£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cd4b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    norm = unicodedata.normalize('NFKD', str(text))\n",
    "    norm = norm.encode('ASCII', 'ignore').decode('utf-8')\n",
    "    norm = re.sub(r'[^\\w\\s]', '')\n",
    "    norm = norm.lower().strip()\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def extract_tpei(tpei_str):\n",
    "    if pd.isna(tpei_str):\n",
    "        return []\n",
    "    \n",
    "    values = tpei_str.split('-')\n",
    "    return {\n",
    "        'teoria': int(values[0]),\n",
    "        'pratica': int(values[1]),\n",
    "        'extensao': int(values[2]),\n",
    "        'individual': int(values[3]),\n",
    "        'total_creditos': int(values[0]) + int(values[1])  # T+P only\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def extract_prereq(recomendacao):\n",
    "    if pd.isna(recomendacao) or recomendacao.strip() == '':\n",
    "        return []\n",
    "\n",
    "    prereqs = []\n",
    "    for part in recomendacao.split(';'):\n",
    "        part = part.strip()\n",
    "        if part:\n",
    "            prereqs.append(part)\n",
    "    return prereqs\n",
    "\n",
    "def extract_cod(prereqs):\n",
    "    if pd.isna(prereqs):\n",
    "        return []\n",
    "    for sigla in prereqs.split(';'):\n",
    "        sigla = sigla.strip()\n",
    "        if sigla:\n",
    "            prereqs.append(sigla)\n",
    "    return prereqs\n",
    "\n",
    "def create_allfeats(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # TPEI\n",
    "    tpei_feats = df['TPEI'].apply(extract_tpei)\n",
    "    df['teoria'] = tpei_feats.apply(lambda x: x['teoria'])\n",
    "    df['pratica'] = tpei_feats.apply(lambda x: x['pratica'])\n",
    "    df['extensao'] = tpei_feats.apply(lambda x: x['extensao'])\n",
    "    df['individual'] = tpei_feats.apply(lambda x: x['individual'])\n",
    "    df['total_creditos'] = tpei_feats.apply(lambda x: x['total_creditos'])\n",
    "\n",
    "    # Ementa\n",
    "    df['ementa_norm'] = df['EMENTA'].apply(normalize_text)\n",
    "    df['objetivos_norm'] = df['OBJETIVOS'].apply(normalize_text)\n",
    "\n",
    "    # PrÃ©-requisitos\n",
    "    df['prerequisites'] = df['RECOMENDAÃ‡ÃƒO'].apply(extract_prereq)\n",
    "    df['num_prerequisites'] = df['prerequisites'].apply(len)\n",
    "    df['codigo'] = df['SIGLA'].apply(extract_cod)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd40eb",
   "metadata": {},
   "source": [
    "## 4. Filtro TPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eab5d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_filtro_tpei(df_pares, df_disciplinas):\n",
    "    print(\"ðŸ”„ Aplicando filtro TPEI...\")\n",
    "    \n",
    "    # Criar dicionÃ¡rio para mapeamento rÃ¡pido de sigla -> crÃ©ditos\n",
    "    creditos_dict = {row['SIGLA']: row['total_creditos'] \n",
    "                    for _, row in df_disciplinas.iterrows() \n",
    "                    if pd.notna(row['SIGLA'])}\n",
    "    \n",
    "    # DataFrame para armazenar pares filtrados\n",
    "    pares_filtrados = []\n",
    "    \n",
    "    # DicionÃ¡rio para armazenar diferenÃ§as de crÃ©ditos\n",
    "    tpei_dif = {}\n",
    "    \n",
    "    # Registrar pares excluÃ­dos para log\n",
    "    pares_excluidos = []\n",
    "    \n",
    "    # Aplicar filtro a cada par\n",
    "    for _, row in df_pares.iterrows():\n",
    "        sigla_a = row['SIGLA_A']  # current\n",
    "        sigla_b = row['SIGLA_B']  # comp\n",
    "        \n",
    "        # Verificar se ambas as siglas existem no dicionÃ¡rio\n",
    "        if sigla_a not in creditos_dict or sigla_b not in creditos_dict:\n",
    "            pares_excluidos.append((sigla_a, sigla_b, \"Sigla nÃ£o encontrada\"))\n",
    "            continue\n",
    "        \n",
    "        creditos_a = creditos_dict[sigla_a]\n",
    "        creditos_b = creditos_dict[sigla_b]\n",
    "        \n",
    "        # Verificar a regra de filtro\n",
    "        if creditos_a > creditos_b:\n",
    "            # Remover par se crÃ©ditos de A > crÃ©ditos de B\n",
    "            pares_excluidos.append((sigla_a, sigla_b, f\"CrÃ©ditos A({creditos_a}) > B({creditos_b})\"))\n",
    "        else:\n",
    "            # Manter o par e calcular a diferenÃ§a\n",
    "            pares_filtrados.append(row)\n",
    "            tpei_dif[(sigla_a, sigla_b)] = creditos_b - creditos_a\n",
    "    \n",
    "    # Criar DataFrame com pares filtrados\n",
    "    df_filtrado = pd.DataFrame(pares_filtrados)\n",
    "    \n",
    "    print(f\"âœ… Filtro TPEI aplicado: {len(df_filtrado)} pares mantidos, {len(pares_excluidos)} excluÃ­dos\")\n",
    "    \n",
    "    return df_filtrado, tpei_dif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e34318",
   "metadata": {},
   "source": [
    "## 5. Filtro de PrÃ©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DiGraph from prerequisite relationships\n",
    "# Generate node2vec embeddings for disciplines\n",
    "# Calculate cosine distance between discipline embeddings\n",
    "# Apply prerequisite similarity threshold\n",
    "# Filter pairs based on prerequisite similarity\n",
    "\n",
    "#  ConstrÃ³i grafo com arestas de prÃ©-requisito com base na coluna RECOMENDACAO\n",
    "def construir_grafo(df: pd.DataFrame) -> nx.DiGraph:\n",
    "    print(\"ðŸ“Œ Construindo grafo de prÃ©-requisitos...\")\n",
    "    G = nx.DiGraph()\n",
    "    total_arestas = 0\n",
    "\n",
    "    # Cria um dicionÃ¡rio de nome normalizado â†’ sigla\n",
    "    mapping = {\n",
    "        normalize_text(row['DISCIPLINA']): row['SIGLA']\n",
    "        for _, row in df.iterrows()\n",
    "        if pd.notna(row['SIGLA'])\n",
    "    }\n",
    "\n",
    "    # Adiciona todos os nÃ³s com metadados\n",
    "    for _, row in df.iterrows():\n",
    "        sigla = row['SIGLA']\n",
    "        nome = row['DISCIPLINA']\n",
    "        if pd.isna(sigla): \n",
    "            continue\n",
    "        G.add_node(sigla, nome=nome)\n",
    "\n",
    "    # Adiciona as arestas baseadas nas recomendaÃ§Ãµes\n",
    "    for _, row in df.iterrows():\n",
    "        curso = row['SIGLA']\n",
    "        if pd.isna(curso):\n",
    "            continue\n",
    "\n",
    "        recs = row.get('RECOMENDACAO', '')\n",
    "        if pd.isna(recs) or not isinstance(recs, str):\n",
    "            continue\n",
    "\n",
    "        for rec in recs.split(';'):\n",
    "            rec = rec.strip()\n",
    "            if not rec:\n",
    "                continue\n",
    "            rec_norm = normalize_text(rec)\n",
    "            if rec_norm in mapping:\n",
    "                prereq = mapping[rec_norm]\n",
    "                if not G.has_edge(prereq, curso):\n",
    "                    G.add_edge(prereq, curso, tipo='pre_requisito')\n",
    "                    total_arestas += 1\n",
    "\n",
    "    print(f\"âœ… Grafo criado com {G.number_of_nodes()} nÃ³s e {total_arestas} arestas.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc787716",
   "metadata": {},
   "source": [
    "## 6. CÃ¡lculo de Score TPEI + PrÃ©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a218d884",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_catboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(features)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Treina CatBoost usando labeled_data\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# CÃ³digo da Larissa\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Otimiza limiar\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# from sklearn.metrics import precision_recall_curve\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m y_pred_proba = model_catboost.predict_proba(X_val)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     24\u001b[39m precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n\u001b[32m     25\u001b[39m f1_scores = \u001b[32m2\u001b[39m * (precision * recall) / (precision + recall + \u001b[32m1e-8\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_catboost' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepara matriz de caracteristicas\n",
    "def prepare_combined_features(discipline_pairs, tpei_diff, prereq_similarities):\n",
    "    features = []\n",
    "    \n",
    "    pares_filtrados = []\n",
    "    \n",
    "    pares_excluidos = []\n",
    "    \n",
    "    for pair in discipline_pairs:\n",
    "        prereq_sim = prereq_similarities.get(pairs, 0.0)\n",
    "        features.append([tpei_diff, prereq_sim])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Treina CatBoost usando labeled_data\n",
    "# CÃ³digo da Larissa\n",
    "\n",
    "\n",
    "\n",
    "# Otimiza limiar\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "y_pred_proba = model_catboost.predict_proba(X_val)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Calcula score de cada par\n",
    "all_features = prepare_combined_features(candidate_pairs, tpei_diff, prereq_similarities)\n",
    "combined_scores = model_catboost.predict_proba(all_features)[:, 1]\n",
    "\n",
    "# Aplica filtro em cada par\n",
    "filtered_pairs = [\n",
    "    pair for pair, score in zip(candidate_pairs, combined_scores)\n",
    "    if score >= optimal_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5bf8a",
   "metadata": {},
   "source": [
    "## 7. Filtro de Ementa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3744b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "parameter without a default follows parameter with a default (505583953.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef similariry_between_DISCIPLINA(cosine_sim, similarity_threshold = 0.75, df):\u001b[39m\n                                                                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m parameter without a default follows parameter with a default\n"
     ]
    }
   ],
   "source": [
    "# Load BERTimbau pre-trained model\n",
    "# Generate embeddings for ementa texts\n",
    "# Apply node2vec to ementa embeddings\n",
    "# Calculate cosine distance between ementa embeddings\n",
    "# Filter based on semantic similarity threshold\n",
    "\n",
    "import re\n",
    "\n",
    "def sao_variantes_simples(nome1, nome2):\n",
    "    base1 = re.sub(r'[\\s\\-]*(laboratÃ³rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome1.strip(), flags=re.IGNORECASE)\n",
    "    base2 = re.sub(r'[\\s\\-]*(laboratÃ³rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome2.strip(), flags=re.IGNORECASE)\n",
    "    return base1.lower() == base2.lower()\n",
    "\n",
    "def similariry_between_DISCIPLINA(cosine_sim, similarity_threshold = 0.75):\n",
    "    df = pd.DataFrame\n",
    "    # Encontrar pares com similaridade â‰¥ 75%\n",
    "    similar_pairs = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):  # Evitar duplicatas (i, j) e (j, i)\n",
    "            nome_i = df.iloc[i]['DISCIPLINA']\n",
    "            nome_j = df.iloc[j]['DISCIPLINA']\n",
    "            if sao_variantes_simples(nome_i, nome_j):\n",
    "                continue  # pula se sÃ£o versÃµes quase idÃªnticas da mesma disciplina\n",
    "            if cosine_sim[i, j] >= similarity_threshold:\n",
    "                similar_pairs.append((nome_i, nome_j, cosine_sim[i, j]))\n",
    "\n",
    "    # Exibir resultados\n",
    "    for pair in similar_pairs:\n",
    "        print(f\"Disciplinas similares: {pair[0]} e {pair[1]} (Similaridade: {pair[2]:.2f})\")\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "model = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
    "embeddings = model.encode(pd.DataFrame['EMENTA_PREPROCESSED'].tolist())\n",
    "cosine_sim_bert = cosine_similarity(embeddings, embeddings)\n",
    "sim_pair = similariry_between_DISCIPLINA(cosine_sim_bert)\n",
    "\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "# Criar modelo SBERT a partir do BERT\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode_mean_tokens=True)\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "embeddings = model.encode(pd.DataFrame['EMENTA_PREPROCESSED'].tolist())\n",
    "cosine_sim_sbert = cosine_similarity(embeddings, embeddings)\n",
    "sim_pair = similariry_between_DISCIPLINA(cosine_sim_sbert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f7907",
   "metadata": {},
   "source": [
    "## 8. CÃ¡lculo de Score Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara caracteristicas\n",
    "def prepare_final_features(pairs, tpei_dif, prereq_sim, ementa_sim):\n",
    "    features = []\n",
    "    for pair in pairs:\n",
    "        prereq = prereq_sim.get(pair, 0.0)\n",
    "        ementa = ementa_sim(pair, 0.0)\n",
    "        features.append([tpei_dif, prereq, ementa])\n",
    "    return np.array(features)\n",
    "\n",
    "# Treino SVM com Kernel RBF\n",
    "# Codigo da Larissa\n",
    "\n",
    "# Calcula limiar Ã³timo\n",
    "y_prob = svm_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_final, y_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "final_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "all_final_features = prepare_final_features(\n",
    "    filtered_pairs, tpei_dif, prereq_sim, ementa_sim\n",
    ")\n",
    "all_scaled = scaler.transform(all_final_features)\n",
    "final_scores = svm_model.predict_proba(all_scaled)[:, 1]\n",
    "\n",
    "#Lista final de pares\n",
    "equivalent_pairs = [\n",
    "    (pair, score) for pair, score in zip(filtered_pairs, final_scores)\n",
    "    if score >= final_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff0ccc",
   "metadata": {},
   "source": [
    "## 9. ExplicaÃ§Ã£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "explainer = shap.KernelExplainer(\n",
    "    svm_model.predict_proba,\n",
    "    X_scaled,\n",
    "    link=\"logit\"\n",
    ")\n",
    "\n",
    "shap_values = explainer.shap_values(all_scaled)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "feature_names = ['DiferenÃ§a de crÃ©ditos', 'Similaridade de prÃ©-requisito', 'Similaridade de ementa']\n",
    "importance = np.abs(shap_values).mean(0)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "def explain_prediction(pair_index, features, shap_values):\n",
    "    feature_values = features[pair_index]\n",
    "    shap_value = shap_values[pair_index]\n",
    "    \n",
    "    explanation = []\n",
    "    for i, (name, value, impact) in enumerate(zip(feature_names, feature_values, shap_value)):\n",
    "        direction = \"positive\" if impact > 0 else \"negative\"\n",
    "        explanation.append(f\"{name}: {value:.3f} ({direction} impact: {abs(impact):.3f})\")\n",
    "    \n",
    "    return \"\\n\".join(explanation)\n",
    "\n",
    "# Generate explanations for all equivalent pairs\n",
    "explanations = []\n",
    "for i, (pair, score) in enumerate(equivalent_pairs):\n",
    "    explanation = f\"Pair: {pair[0]} - {pair[1]}\\n\"\n",
    "    explanation += f\"Equivalence Score: {score:.3f}\\n\"\n",
    "    explanation += \"Feature Contributions:\\n\"\n",
    "    explanation += explain_prediction(i, all_final_features, shap_values)\n",
    "    explanations.append(explanation)\n",
    "# Initialize SHAP explainer for SVM model\n",
    "# Calculate SHAP values for each prediction\n",
    "# Generate feature importance rankings\n",
    "# Create individual prediction explanations\n",
    "# Prepare text explanations for results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79476c28",
   "metadata": {},
   "source": [
    "## 10. VisualizaÃ§Ãµes dos Resultados\n",
    "\n",
    "### 10.1 Matriz de visualizaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "df = pd.DataFrame\n",
    "\n",
    "# Adicionar nÃ³s (disciplinas)\n",
    "for sigla in df['DISCIPLINA'].unique():\n",
    "    G.add_node(sigla)\n",
    "\n",
    "# Modificar a criaÃ§Ã£o de arestas\n",
    "for pair in similar_pairs:\n",
    "    disciplina_a, disciplina_b, similarity = pair\n",
    "    if similarity >= 0.8:  # Ajuste o threshold aqui\n",
    "        G.add_edge(disciplina_a, disciplina_b, weight=similarity)\n",
    "\n",
    "        # Calcular posiÃ§Ãµes dos nÃ³s\n",
    "        pos = nx.kamada_kawai_layout(G, weight='weight')  # Usa o peso (similaridade) para organizar\n",
    "\n",
    "        # Criar traÃ§os para arestas\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])  # None para separar linhas\n",
    "            edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line= dict(width=1, color='#888'),  # Espessura e cor das arestas\n",
    "            hoverinfo='none',\n",
    "            mode='lines')\n",
    "\n",
    "# Criar traÃ§os para nÃ³s\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_text = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)  # Texto ao passar o mouse\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "  x=node_x, y=node_y,\n",
    "  mode='markers+text',\n",
    "  text=node_text,\n",
    "  textposition=\"top center\",\n",
    "  hoverinfo='text',\n",
    "  marker=dict(\n",
    "      showscale=True,\n",
    "      colorscale='YlGnBu',\n",
    "      size=15,\n",
    "      color=[],  # Pode ser usado para codificar cores por comunidade\n",
    "      line=dict(width=2, color='black'))\n",
    ")\n",
    "\n",
    "# Criar figura\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "    layout=go.Layout(\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=0, l=0, r=0, t=0),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    ")\n",
    "\n",
    "# Adicionar interatividade (exibir sigla ao passar o mouse)\n",
    "fig.update_traces(textposition='top center', hoverinfo='text')\n",
    "\n",
    "from networkx.algorithms import community\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "# Atribuir cores diferentes a cada comunidade\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create DiGraph visualization showing prerequisite relationships\n",
    "# Color nodes based on equivalence status\n",
    "# Highlight connected discipline pairs\n",
    "# Add node labels and edge weights\n",
    "# Save graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d967",
   "metadata": {},
   "source": [
    "### 10.2 VisualizaÃ§Ã£o por mapa de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd8245ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine_sim_bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Criar matriz de similaridade como DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m similarity_df = pd.DataFrame(cosine_sim_bert, index=df[\u001b[33m'\u001b[39m\u001b[33mDISCIPLINA\u001b[39m\u001b[33m'\u001b[39m], columns=df[\u001b[33m'\u001b[39m\u001b[33mDISCIPLINA\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Plotar heatmap\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m10\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'cosine_sim_bert' is not defined"
     ]
    }
   ],
   "source": [
    "# Criar matriz de similaridade como DataFrame\n",
    "similarity_df = pd.DataFrame(cosine_sim_bert, index=df['DISCIPLINA'], columns=df['DISCIPLINA'])\n",
    "\n",
    "# Plotar heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_df, cmap='YlGnBu', annot=False, mask=(similarity_df < 0.5))\n",
    "plt.title('Mapa de Calor de Similaridade entre Ementas')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Generate SHAP summary plot for feature importance\n",
    "# Create dependence plots for key features\n",
    "# Visualize force plots for individual predictions\n",
    "# Export plots for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcaaf6",
   "metadata": {},
   "source": [
    "### 10.3 Matrizes de ConfusÃ£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbad8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for model performance\n",
    "# Create heatmap visualization of confusion matrix\n",
    "# Add precision, recall, and F1 scores\n",
    "# Generate performance metrics report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868a7f5",
   "metadata": {},
   "source": [
    "## 11. ExportaÃ§Ã£o de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f65c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered results to TSV file\n",
    "# Export model performance metrics\n",
    "# Save visualizations in specified formats\n",
    "# Generate final summary report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "node2vec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
