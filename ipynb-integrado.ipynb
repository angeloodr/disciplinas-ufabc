{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343744a0",
   "metadata": {},
   "source": [
    "# Sistema de DetecÃ§Ã£o de EquivalÃªncia de Disciplinas\n",
    "\n",
    "# Notebook de verificaÃ§Ã£o da similaridade entre ementas de disciplinas\n",
    "\n",
    "## IntroduÃ§Ã£o:\n",
    "O estudante da UFABC passa muito tempo jÃ¡ em sua graduaÃ§Ã£o para terminar as disciplinas do seu BI e do pÃ³s-BI e isso afeta principalmente os alunos de cursos mais concorridos como Ã© o de computaÃ§Ã£o, em que as disciplinas podem ter mais de 150% de requisiÃ§Ã£o. Tendo em vista isso, na UFABC temos dois processos jÃ¡ estruturados que Ã© o processo de covalidaÃ§Ã£o e de equivalÃªncia, normatizados nas resoluÃ§Ãµes ConsEPE nÂº 157/2013 e CG NÂº 023/2019 respectivamente. CovalidaÃ§Ã£o Ã© um processo interno da UFABC que Ã© basicamente para a transiÃ§Ã£o de projetos pedagÃ³gicos, de forma que o(a) estudante consegue integralizar o curso em um PPC antigo com disciplinas novas e a equivalÃªncia Ã© um processo que uma disciplina de fora pode ter alguma similaridade de uma disciplina da ufabc e do curso que vocÃª quer se formar. Assim estÃ¡ presente na ResoluÃ§Ã£o CG NÂº 023/2019 o seguinte:\n",
    "\n",
    ">Art. 4Âº Consistem em requisitos para a dispensa por equivalÃªncia, para disciplinas\n",
    "cursadas no Brasil:\n",
    "\n",
    ">> I. a carga horÃ¡ria total da disciplina cursada deve ser igual ou maior Ã  carga horÃ¡ria da que se pede equivalÃªncia;\n",
    "\n",
    ">> II. o conteÃºdo da disciplina cursada deve ser compatÃ­vel e correspondente a, no mÃ­nimo, 75% (setenta e cinco por cento) do conteÃºdo daquela de que se pede equivalÃªncia, considerando-se teoria e prÃ¡tica, quando for o caso. \n",
    "\n",
    ">>>ParÃ¡grafo Ãºnico: Excepcionalmente, e mediante justificativa, a coordenaÃ§Ã£o de curso pode autorizar equivalÃªncias que cumpram parcialmente estes requisito\n",
    "\n",
    "Utilizando essa base das normativas e os catÃ¡logos de disciplinas da UFABC, objetivamos gerar um sistema de prÃ©-avaliaÃ§Ã£o de disciplinas com alta chance de convalidaÃ§Ã£o, reduzindo o espaÃ§o de busca dos tÃ©cnicos responsaveis pela aprovaÃ§Ã£o de pares de disciplinas com convalidaÃ§Ã£o e garantindo a inclusÃ£o de todas as disciplinas na anÃ¡lise. Os benefÃ­cio desta proposta sÃ£o a economia de recursos, maior integralizaÃ§Ã£o de diferentes PPCs e promove efetivamente a interdisciplinaridade, fundamento da UFAB, uma vez que os cursos poderiam ofertar a mesma disciplina com diferentes enfoques no mesmo quadrimestre, melhorando a qualidade do ensino. Os recursos poupados sÃ£o tanto na carga de trabalho dos tÃ©cnicos quanto recursos computacionais (realizada manualmente, ess tarefa tem um complexidade O(n^2)), requerindo anÃ¡lise manual apenas das disciplinas que se encaixam nos critÃ©rios das resoluÃ§Ãµes ConsEPE nÂº 157/2013 e CG NÂº 023/2019 e com alta probabilidade de validaÃ§Ã£o. \n",
    "\n",
    "Dessa forma propomos uma anÃ¡lise de similaridade semantica e relacional entre as ementas, os prÃ©-requisitos das disciplinas da UFABC e dos valores de TPEI, para fim de prÃ©-avaliaÃ§Ã£o de equivalÃªncias entre as diciplinas da universidade cumprindo com o Art. 4Â° da resoluÃ§Ã£o CG NÂº 023/2019. Assim. em disciplinas que verificarmos que existe uma similaridade equivalente a de pares de disciplinas atualmente validadas (similaridade maior ou igual a 75% e que cumpre a quantidade de creditos da outra disciplina), poderemos gerar listas de pares para aprovaÃ§Ã£o manual em ordem de similaridde. O benefÃ­cio proporcionado pela maior integraÃ§Ã£o do sistema Ã© difÃ­cil de mensurar mas, alÃ©m de afetar todes es envolvides no processo discente, o alinhamento dessa soluÃ§Ã£o com os sistemas vigentes da universidade amplificam seu impacto e condiÃ§Ãµes em que ele se manifesta.\n",
    "\n",
    "## Objetivos\n",
    "1. Fazer uma proposta de equivalÃªncia interna de disciplinas a partir da anÃ¡lise de similaridade semÃ¢ntica entre as ementas das disciplinas da UFABC da GraduaÃ§Ã£o e da quantidade de TPEI que elas tem\n",
    "2. Reduzir o espaÃ§o de busca de O(nÂ²) para aproximadamente O(n*log(n)) a partir da aplicaÃ§Ã£o de filtros de TPEI para quantidade de crÃ©ditos das disciplinas, de forma sÃ³ comparar as diciplinas que tem a quantidade de crÃ©ditos iguais\n",
    "\n",
    "## MÃ©todos\n",
    "1. Embedding semÃ¢ntico do conteÃºdo dos Objetivos e Ementas das disciplinas utilizando BERTimbau\n",
    "2. CÃ¡lculo de similaridade cosseno para similaridade entre os textos \n",
    "3. Kmeans nos embeddings para determinar agrupamentos de disciplinas ajudando no processo de equivalÃªncia\n",
    "4. Grafo relacional dos PrÃ©-Requisitos entre disciplinas\n",
    "5. CÃ¡lculo de similaridade utiliando distÃ¢ncias de jaccard\n",
    "6. OtimizaÃ§Ã£o dos hiperparÃ¢metros dos filtros de reduÃ§Ã£o do espaÃ§o de busca utilizando modelos de Ã¡rvores aleatÃ³rias \n",
    "\n",
    "## 1. Imports e ConfiguraÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza imports necessÃ¡rios\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import unicodedata\n",
    "import requests\n",
    "from io import StringIO\n",
    "from node2vec import Node2Vec\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85da40",
   "metadata": {},
   "source": [
    "## 2. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f4b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixa o catÃ¡logo da UFABC direto do RepositÃ³rio no GitHub\n",
    "def carregar_catalogo():\n",
    "    url = \"https://raw.githubusercontent.com/angeloodr/disciplinas-ufabc/main/catalogo_disciplinas_graduacao_2024_2025.tsv\"\n",
    "    print(\"ðŸ”„ Baixando catÃ¡logo de disciplinas...\")\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    df = pd.read_csv(StringIO(resp.text), sep='\\t')\n",
    "\n",
    "    # Normaliza os nomes das colunas\n",
    "    df.columns = [\n",
    "        normalize_str(col).upper().replace(' ', '_')\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    print(\"âœ… Download bem-sucedido!\")\n",
    "    print(\"ðŸ“ Colunas disponÃ­veis:\", df.columns.tolist())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7a644",
   "metadata": {},
   "source": [
    "## 3. Preprocessamento e preparaÃ§Ã£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    '''Normaliza o texto removendo acentuaÃ§Ã£o, pontuaÃ§Ã£o e stopwords. AlÃ©m disso, aplica stemming, reduzindo a palavra ao seu radical.'''\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    norm = unicodedata.normalize('NFKD', str(text))\n",
    "    norm = norm.encode('ASCII', 'ignore').decode('utf-8')\n",
    "    norm = re.sub(r'[^\\w\\s]', '', str(text).lower())\n",
    "    norm = norm.lower().strip()\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def extract_tpei(tpei_str):\n",
    "    '''Extrai os valores de TPEI (Teoria, PrÃ¡tica, ExtensÃ£o e Individual) de uma string no formato \"T-P-E-I\" e retorna um dicionÃ¡rio com os valores.\n",
    "        Se a string for invÃ¡lida, retorna um dicionÃ¡rio vazio.'''\n",
    "    if pd.isna(tpei_str):\n",
    "        return []\n",
    "    \n",
    "    values = tpei_str.split('-')\n",
    "    return {\n",
    "        'teoria': int(values[0]),\n",
    "        'pratica': int(values[1]),\n",
    "        'extensao': int(values[2]),\n",
    "        'individual': int(values[3]),\n",
    "        'total_creditos': int(values[0]) + int(values[1])  # T+P only\n",
    "    }\n",
    "\n",
    "def extract_prereq(recomendacao):\n",
    "    '''Extrai os prÃ©-requisitos de uma string no formato \"Disciplina1; Disciplina2; ...\" e retorna uma lista com os cÃ³digos das disciplinas.\n",
    "        Se a string for invÃ¡lida, retorna uma lista vazia.'''\n",
    "    if pd.isna(recomendacao) or recomendacao.strip() == '' or recomendacao == 'NÃ£o hÃ¡':\n",
    "        return []\n",
    "\n",
    "    prereqs = []\n",
    "    for part in recomendacao.split(';'):\n",
    "        part = part.strip()\n",
    "        if part:\n",
    "            prereqs.append(part)\n",
    "    return prereqs\n",
    "\n",
    "def extract_cod(prereqs):\n",
    "    '''Extrai o cÃ³digo da disciplina de uma string no formato \"SIGLA\" e retorna uma lista com o cÃ³digo.\n",
    "        Se a string for invÃ¡lida, retorna uma lista vazia.'''\n",
    "    if pd.isna(prereqs):\n",
    "        return []\n",
    "    for sigla in prereqs.split(';'):\n",
    "        sigla = sigla.strip()\n",
    "        if sigla:\n",
    "            prereqs.append(sigla)\n",
    "    return prereqs\n",
    "\n",
    "def create_allfeats(df):\n",
    "    '''Cria todas as features necessÃ¡rias para o modelo de recomendaÃ§Ã£o. Retorna um DataFrame com as features criadas.'''\n",
    "    df = df.copy()\n",
    "\n",
    "    # TPEI\n",
    "    tpei_feats = df['TPEI'].apply(extract_tpei)\n",
    "    df['teoria'] = tpei_feats.apply(lambda x: x['teoria'])\n",
    "    df['pratica'] = tpei_feats.apply(lambda x: x['pratica'])\n",
    "    df['extensao'] = tpei_feats.apply(lambda x: x['extensao'])\n",
    "    df['individual'] = tpei_feats.apply(lambda x: x['individual'])\n",
    "    df['total_creditos'] = tpei_feats.apply(lambda x: x['total_creditos'])\n",
    "\n",
    "    # Ementa\n",
    "    df['ementa_norm'] = df['EMENTA'].apply(normalize_text)\n",
    "    df['objetivos_norm'] = df['OBJETIVOS'].apply(normalize_text)\n",
    "\n",
    "    # PrÃ©-requisitos\n",
    "    df['prerequisites'] = df['RECOMENDAÃ‡ÃƒO'].apply(extract_prereq)\n",
    "    df['num_prerequisites'] = df['prerequisites'].apply(len)\n",
    "    df['codigo'] = df['SIGLA'].apply(extract_cod)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Baixando catÃ¡logo de disciplinas...\n",
      "âœ… Download bem-sucedido!\n",
      "ðŸ“ Colunas disponÃ­veis: ['SIGLA', 'DISCIPLINA', 'TPEI', 'RECOMENDAÃ‡ÃƒO', 'OBJETIVOS', 'METODOLOGIA EXTENSIONISTA', 'EMENTA', 'BIBLIOGRAFIA BÃSICA', 'BIBLIOGRAFIA COMPLEMENTAR']\n"
     ]
    }
   ],
   "source": [
    "#### inicializar os dados\n",
    "#carregar o catÃ¡logo de disciplinas\n",
    "df = carregar_catalogo()\n",
    "# realizar o prÃ©-processamento dos dados criando as features\n",
    "df = create_allfeats(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd40eb",
   "metadata": {},
   "source": [
    "## 4. Filtro TPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eab5d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_filtro_tpei(df_pares, df_disciplinas):\n",
    "    print(\"ðŸ”„ Aplicando filtro TPEI...\")\n",
    "    \n",
    "    # Criar dicionÃ¡rio para mapeamento rÃ¡pido de sigla -> crÃ©ditos\n",
    "    creditos_dict = {row['SIGLA']: row['total_creditos'] \n",
    "                    for _, row in df_disciplinas.iterrows() \n",
    "                    if pd.notna(row['SIGLA'])}\n",
    "    \n",
    "    # DataFrame para armazenar pares filtrados\n",
    "    pares_filtrados = []\n",
    "    \n",
    "    # DicionÃ¡rio para armazenar diferenÃ§as de crÃ©ditos\n",
    "    tpei_dif = {}\n",
    "    \n",
    "    # Registrar pares excluÃ­dos para log\n",
    "    pares_excluidos = []\n",
    "    \n",
    "    # Aplicar filtro a cada par\n",
    "    for _, row in df_pares.iterrows():\n",
    "        sigla_a = row['SIGLA_A']  # current\n",
    "        sigla_b = row['SIGLA_B']  # comp\n",
    "        \n",
    "        # Verificar se ambas as siglas existem no dicionÃ¡rio\n",
    "        if sigla_a not in creditos_dict or sigla_b not in creditos_dict:\n",
    "            pares_excluidos.append((sigla_a, sigla_b, \"Sigla nÃ£o encontrada\"))\n",
    "            continue\n",
    "        \n",
    "        creditos_a = creditos_dict[sigla_a]\n",
    "        creditos_b = creditos_dict[sigla_b]\n",
    "        \n",
    "        # Verificar a regra de filtro\n",
    "        if creditos_a > creditos_b:\n",
    "            # Remover par se crÃ©ditos de A > crÃ©ditos de B\n",
    "            pares_excluidos.append((sigla_a, sigla_b, f\"CrÃ©ditos A({creditos_a}) > B({creditos_b})\"))\n",
    "        else:\n",
    "            # Manter o par e calcular a diferenÃ§a\n",
    "            pares_filtrados.append(row)\n",
    "            tpei_dif[(sigla_a, sigla_b)] = creditos_b - creditos_a\n",
    "    \n",
    "    # Criar DataFrame com pares filtrados\n",
    "    df_filtrado = pd.DataFrame(pares_filtrados)\n",
    "    \n",
    "    print(f\"âœ… Filtro TPEI aplicado: {len(df_filtrado)} pares mantidos, {len(pares_excluidos)} excluÃ­dos\")\n",
    "    \n",
    "    return df_filtrado, tpei_dif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e34318",
   "metadata": {},
   "source": [
    "## 5. Filtro de PrÃ©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DiGraph from prerequisite relationships\n",
    "# Generate node2vec embeddings for disciplines\n",
    "# Calculate cosine distance between discipline embeddings\n",
    "# Apply prerequisite similarity threshold\n",
    "# Filter pairs based on prerequisite similarity\n",
    "\n",
    "#  ConstrÃ³i grafo com arestas de prÃ©-requisito com base na coluna RECOMENDACAO\n",
    "def construir_grafo(df: pd.DataFrame) -> nx.DiGraph:\n",
    "    print(\"ðŸ“Œ Construindo grafo de prÃ©-requisitos...\")\n",
    "    G = nx.DiGraph()\n",
    "    total_arestas = 0\n",
    "\n",
    "    # Cria um dicionÃ¡rio de nome normalizado â†’ sigla\n",
    "    mapping = {\n",
    "        normalize_text(row['DISCIPLINA']): row['SIGLA']\n",
    "        for _, row in df.iterrows()\n",
    "        if pd.notna(row['SIGLA'])\n",
    "    }\n",
    "\n",
    "    # Adiciona todos os nÃ³s com metadados\n",
    "    for _, row in df.iterrows():\n",
    "        sigla = row['SIGLA']\n",
    "        nome = row['DISCIPLINA']\n",
    "        if pd.isna(sigla): \n",
    "            continue\n",
    "        G.add_node(sigla, nome=nome)\n",
    "\n",
    "    # Adiciona as arestas baseadas nas recomendaÃ§Ãµes\n",
    "    for _, row in df.iterrows():\n",
    "        curso = row['SIGLA']\n",
    "        if pd.isna(curso):\n",
    "            continue\n",
    "\n",
    "        recs = row.get('RECOMENDACAO', '')\n",
    "        if pd.isna(recs) or not isinstance(recs, str):\n",
    "            continue\n",
    "\n",
    "        for rec in recs.split(';'):\n",
    "            rec = rec.strip()\n",
    "            if not rec:\n",
    "                continue\n",
    "            rec_norm = normalize_text(rec)\n",
    "            if rec_norm in mapping:\n",
    "                prereq = mapping[rec_norm]\n",
    "                if not G.has_edge(prereq, curso):\n",
    "                    G.add_edge(prereq, curso, tipo='pre_requisito')\n",
    "                    total_arestas += 1\n",
    "\n",
    "    print(f\"âœ… Grafo criado com {G.number_of_nodes()} nÃ³s e {total_arestas} arestas.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc68d6",
   "metadata": {},
   "source": [
    "### Processamento do Grafo: Rede Direcionada e Embeddings Estruturais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remove ciclos do grafo para que seja um DAG (necessÃ¡rio para cÃ¡lculo de profundidade)\n",
    "def remover_ciclos(G: nx.DiGraph) -> nx.DiGraph:\n",
    "    print(\"ðŸ” Removendo ciclos (modo eficiente)...\")\n",
    "    G_copy = G.copy()\n",
    "    removidos = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ciclo = nx.find_cycle(G_copy, orientation='original')\n",
    "            if ciclo:\n",
    "                # Remove a primeira aresta do ciclo\n",
    "                u, v, _ = ciclo[0]\n",
    "                G_copy.remove_edge(u, v)\n",
    "                removidos += 1\n",
    "    except nx.NetworkXNoCycle:\n",
    "        pass  # NÃ£o hÃ¡ mais ciclos\n",
    "\n",
    "    print(f\"âœ… Ciclos removidos: {removidos}\")\n",
    "    return G_copy\n",
    "\n",
    "# Gera embeddings estruturais com Node2Vec\n",
    "def gerar_embeddings_node2vec(G: nx.DiGraph, dimensions=64, walk_length=10, num_walks=50, workers=1):\n",
    "    print(\"ðŸ”„ Gerando embeddings estruturais (Node2Vec)...\")\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers)\n",
    "    model = node2vec.fit(window=5, min_count=1, batch_words=4)\n",
    "    embeddings = {node: model.wv[node] for node in G.nodes()}\n",
    "    print(\"âœ… Embeddings gerados!\")\n",
    "    return embeddings\n",
    "\n",
    "# Calcula profundidade de cada nÃ³ no grafo (camada curricular)\n",
    "def calcular_profundidade(G: nx.DiGraph):\n",
    "    print(\"ðŸ“ Calculando profundidade no grafo (DAG)...\")\n",
    "    profundidades = {}\n",
    "    for node in nx.topological_sort(G):\n",
    "        preds = list(G.predecessors(node))\n",
    "        if not preds:\n",
    "            profundidades[node] = 0\n",
    "        else:\n",
    "            profundidades[node] = max(profundidades[p] for p in preds) + 1\n",
    "    print(\"âœ… Profundidade calculada!\")\n",
    "    return profundidades\n",
    "\n",
    "# Salva os embeddings em CSV\n",
    "def salvar_embeddings_csv(embeddings, caminho=\"embeddings_node2vec.csv\"):\n",
    "    df = pd.DataFrame.from_dict(embeddings, orient='index')\n",
    "    df.index.name = 'DISCIPLINA'\n",
    "    df.to_csv(caminho)\n",
    "    print(f\"ðŸ’¾ Embeddings salvos em: {caminho}\")\n",
    "\n",
    "# Fluxo principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Etapa 1 - Carregar catÃ¡logo\n",
    "    df = carregar_catalogo()\n",
    "\n",
    "    # Etapa 2 - Construir grafo\n",
    "    grafo = construir_grafo(df)\n",
    "    nx.write_graphml(grafo, \"grafo_pre_requisitos.graphml\")\n",
    "    print(\"ðŸ“ Arquivo salvo: grafo_pre_requisitos.graphml\")\n",
    "\n",
    "    # Etapa 3 - Gerar embeddings estruturais\n",
    "    embeddings = gerar_embeddings_node2vec(grafo)\n",
    "    salvar_embeddings_csv(embeddings)\n",
    "\n",
    "    # Etapa 4 - Remover ciclos e calcular profundidade\n",
    "    grafo_sem_ciclos = remover_ciclos(grafo)\n",
    "    profundidades = calcular_profundidade(grafo_sem_ciclos)\n",
    "\n",
    "    # Etapa 5 - Salvar profundidades em TXT\n",
    "    with open(\"profundidade_nos.txt\", \"w\") as f:\n",
    "        for node, prof in profundidades.items():\n",
    "            f.write(f\"{node}: {prof}\\n\")\n",
    "    print(\"ðŸ“ Profundidades salvas em: profundidade_nos.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0352f6",
   "metadata": {},
   "source": [
    "### Com o grafo pronto e os dados carregados, aplicaremos o Ãndice de Jaccard e outras medidas de similaridade para analisar as conexÃµes entre as disciplinas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b161e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard entre dois conjuntos\n",
    "def jaccard_similarity(set1, set2):\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "# Aplica Jaccard entre predecessores ou sucessores\n",
    "def similaridade_jaccard(G, a, b, tipo='predecessor'):\n",
    "    try:\n",
    "        if tipo == 'predecessor':\n",
    "            set_a = set(G.predecessors(a))\n",
    "            set_b = set(G.predecessors(b))\n",
    "        else:\n",
    "            set_a = set(G.successors(a))\n",
    "            set_b = set(G.successors(b))\n",
    "        return jaccard_similarity(set_a, set_b)\n",
    "    except nx.NetworkXError:\n",
    "        return 0.0\n",
    "    \n",
    "# Similaridade de profundidade\n",
    "def similaridade_profundidade(prof, a, b):\n",
    "    if a not in prof or b not in prof:\n",
    "        return 0.0\n",
    "    max_p = max(prof.values())\n",
    "    if max_p == 0:\n",
    "        return 1.0\n",
    "    return 1.0 - abs(prof[a] - prof[b]) / max_p\n",
    "\n",
    "# Similaridade por Node2Vec (cosseno)\n",
    "def similaridade_node2vec(embeddings, a, b):\n",
    "    if a not in embeddings.index or b not in embeddings.index:\n",
    "        return 0.0\n",
    "    vec_a = embeddings.loc[a].values.reshape(1, -1)\n",
    "    vec_b = embeddings.loc[b].values.reshape(1, -1)\n",
    "    return cosine_similarity(vec_a, vec_b)[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a62330",
   "metadata": {},
   "source": [
    "### CombinaÃ§Ã£o das Similaridades em um Score Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6325b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina mÃºltiplas similaridades em score final\n",
    "def similaridade_combinada(G, embeddings, profundidades, a, b, pesos=None):\n",
    "    if pesos is None:\n",
    "        pesos = {\n",
    "            'jaccard_pred': 1.0,\n",
    "            'jaccard_succ': 1.0,\n",
    "            'profundidade': 1.0,\n",
    "            'node2vec': 1.0,\n",
    "        }\n",
    "\n",
    "    sim_jaccard_pred = similaridade_jaccard(G, a, b, tipo='predecessor')\n",
    "    sim_jaccard_succ = similaridade_jaccard(G, a, b, tipo='successor')\n",
    "    sim_profundidade = similaridade_profundidade(profundidades, a, b)\n",
    "    sim_node2vec = similaridade_node2vec(embeddings, a, b)\n",
    "\n",
    "    total_peso = sum(pesos.values())\n",
    "    score = (\n",
    "        pesos['jaccard_pred'] * sim_jaccard_pred +\n",
    "        pesos['jaccard_succ'] * sim_jaccard_succ +\n",
    "        pesos['profundidade'] * sim_profundidade +\n",
    "        pesos['node2vec'] * sim_node2vec\n",
    "    ) / total_peso\n",
    "\n",
    "    return {\n",
    "        \"score_combinado\": score,\n",
    "        \"jaccard_pred\": sim_jaccard_pred,\n",
    "        \"jaccard_succ\": sim_jaccard_succ,\n",
    "        \"profundidade\": sim_profundidade,\n",
    "        \"node2vec\": sim_node2vec\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b286240",
   "metadata": {},
   "source": [
    "###  GeraÃ§Ã£o do arquivo de similaridades filtradas (.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d611736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrita incremental com filtro de profundidade e TPEI\n",
    "if __name__ == \"__main__\":\n",
    "    grafo = carregar_grafo()\n",
    "    embeddings = carregar_embeddings()\n",
    "    profundidades = carregar_profundidades()\n",
    "\n",
    "    caminho_saida = \"similaridades_disciplinas_filtrado.tsv\"\n",
    "    with open(caminho_saida, mode=\"w\", newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out, delimiter=\"\\t\")\n",
    "        # CabeÃ§alho\n",
    "        writer.writerow([\n",
    "            \"disciplina_a\", \"disciplina_b\", \"score_combinado\",\n",
    "            \"jaccard_pred\", \"jaccard_succ\", \"profundidade\", \"node2vec\"\n",
    "        ])\n",
    "\n",
    "        # LaÃ§o otimizado com filtros\n",
    "        for a in grafo.nodes():\n",
    "            for b in grafo.nodes():\n",
    "                if a == b:\n",
    "                    continue\n",
    "\n",
    "                # Filtro por profundidade (ex: 2 nÃ­veis no mÃ¡ximo)\n",
    "                if abs(profundidades.get(a, 0) - profundidades.get(b, 0)) > 2:\n",
    "                    continue\n",
    "\n",
    "                sim = similaridade_combinada(grafo, embeddings, profundidades, a, b)\n",
    "                writer.writerow([\n",
    "                    a, b, sim[\"score_combinado\"],\n",
    "                    sim[\"jaccard_pred\"], sim[\"jaccard_succ\"],\n",
    "                    sim[\"profundidade\"], sim[\"node2vec\"]\n",
    "                ])\n",
    "\n",
    "    print(f\"âœ… Arquivo '{caminho_saida}' gerado com sucesso.\")\n",
    "\n",
    "    # AVISO: O ARQUIVO .TSV CONTENDO O RESULTADO DAS ANÃLISES PODE DEMORAR PARA SER GERADO DEPENDENDO DA MÃQUINA UTILIZADA PARA A EXECUÃ‡ÃƒO!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc787716",
   "metadata": {},
   "source": [
    "## 6. CÃ¡lculo de Score TPEI + PrÃ©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a218d884",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_catboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(features)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Treina CatBoost usando labeled_data\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# CÃ³digo da Larissa\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Otimiza limiar\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# from sklearn.metrics import precision_recall_curve\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m y_pred_proba = model_catboost.predict_proba(X_val)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     24\u001b[39m precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n\u001b[32m     25\u001b[39m f1_scores = \u001b[32m2\u001b[39m * (precision * recall) / (precision + recall + \u001b[32m1e-8\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_catboost' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepara matriz de caracteristicas\n",
    "def prepare_combined_features(discipline_pairs, tpei_diff, prereq_similarities):\n",
    "    features = []\n",
    "    \n",
    "    pares_filtrados = []\n",
    "    \n",
    "    pares_excluidos = []\n",
    "    \n",
    "    for pair in discipline_pairs:\n",
    "        prereq_sim = prereq_similarities.get(pairs, 0.0)\n",
    "        features.append([tpei_diff, prereq_sim])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Treina CatBoost usando labeled_data\n",
    "# CÃ³digo da Larissa\n",
    "\n",
    "\n",
    "\n",
    "# Otimiza limiar\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "y_pred_proba = model_catboost.predict_proba(X_val)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Calcula score de cada par\n",
    "all_features = prepare_combined_features(candidate_pairs, tpei_diff, prereq_similarities)\n",
    "combined_scores = model_catboost.predict_proba(all_features)[:, 1]\n",
    "\n",
    "# Aplica filtro em cada par\n",
    "filtered_pairs = [\n",
    "    pair for pair, score in zip(candidate_pairs, combined_scores)\n",
    "    if score >= optimal_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5bf8a",
   "metadata": {},
   "source": [
    "# Sentence BERT\n",
    "\n",
    "O BERT Ã© um pacote que cria embeddings de palavras a partir de uma rede jÃ¡ treinada. Os embeddings carregam no seus valores o conteudo semantico de cada palavra a partir de uma representaÃ§Ã£o vetorial de cada palavra\n",
    "\n",
    "O Sentence BERT ou SBERT utiliza esses embedings de cada palavra e calula o significado mÃ©dio, por exemplo, de cada frase, de forma que temos uma noÃ§Ã£o do que cada sentenÃ§a significa.\n",
    "\n",
    "existem 1353 disciplinas e no primeiro momento calculamos a similaridade cosseno entre todos os pares. Assim a cossine_sim_bert terÃ¡ a dimensÃ£o 1353x1353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3744b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sao_variantes_simples(nome1, nome2):\n",
    "    base1 = re.sub(r'[\\s\\-]*(laboratÃ³rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome1.strip(), flags=re.IGNORECASE)\n",
    "    base2 = re.sub(r'[\\s\\-]*(laboratÃ³rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome2.strip(), flags=re.IGNORECASE)\n",
    "    return base1.lower() == base2.lower()\n",
    "\n",
    "def similariry_between_DISCIPLINA(dataframe, cosine_sim, similarity_threshold = 0.75):\n",
    "    '''Encontrar pares com similaridade â‰¥ 75% e faz o print dos nomes das disciplinas.\n",
    "        dataframe: DataFrame com os dados das disciplinas\n",
    "        cosine_sim: matriz de similaridade entre as disciplinas\n",
    "        similarity_threshold: limiar de similaridade (default: 0.75)'''\n",
    "\n",
    "    similar_pairs = []\n",
    "    n = len(dataframe)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):  # Evitar duplicatas (i, j) e (j, i)\n",
    "            nome_i = dataframe.iloc[i]['SIGLA']\n",
    "            nome_j = dataframe.iloc[j]['SIGLA']\n",
    "            if sao_variantes_simples(nome_i, nome_j):\n",
    "                continue  # pula se sÃ£o versÃµes quase idÃªnticas da mesma disciplina\n",
    "            if cosine_sim[i, j] >= similarity_threshold:\n",
    "                similar_pairs.append((nome_i, nome_j, cosine_sim[i, j]))\n",
    "\n",
    "    # Exibir resultados\n",
    "    for pair in similar_pairs:\n",
    "        print(f\"Disciplinas similares: {pair[0]} e {pair[1]} (Similaridade: {pair[2]:.2f})\")\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "\n",
    "# Criar modelo SBERT para o portuguÃªs para a coluna, usando o modelo BERT prÃ©-treinado para portuguÃªs\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Definir o modelo de pooling para agregar os embeddings. O pooling combina os embeddings de palavras em um Ãºnico vetor\n",
    "# Usando o modo de pooling mÃ©dio, que calcula a mÃ©dia dos embeddings de palavras\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True)\n",
    "\n",
    "\n",
    "# Estancia o modelo SBERT com o modelo de embedding e o modelo de pooling\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "# Calcula embeddings para as EMENTAS\n",
    "embeddings = model.encode(\n",
    "    df['ementa_norm'].tolist(),\n",
    "    normalize_embeddings=True  # garante que todos os vetores tÃªm norma 1\n",
    ")\n",
    "\n",
    "#calcular a similaridade entre os embeddings usando a similaridade do cosseno\n",
    "cosine_sim_sbert = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "# Fazer o print da similaridade entre as disciplinas com o threshold de 0.75\n",
    "sim_pair = similariry_between_DISCIPLINA(df, cosine_sim_sbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d5a30",
   "metadata": {},
   "source": [
    "## K-means \n",
    "\n",
    "vamos rodar o K-means para determinar se existe algum cluster de disciplinas que poderiamos agrupar. Ã‰ uma outra forma de entender similaridade agora visto com agrupamento de elemntos semelhantes para pensar as equivalencias internas. \n",
    "\n",
    "ComeÃ§amos por determinar os K otimo para esse mÃ©todo utilizando o metodo do cotovelo e o metodo da estatitica de gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964df801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazer um kmeans para encontrar o nÃºmero ideal de clusters utilizando o mÃ©todo do cotovelo\n",
    "# O mÃ©todo do cotovelo Ã© uma tÃ©cnica usada para determinar o nÃºmero ideal de clusters em um conjunto de dados\n",
    "\n",
    "def find_optimal_k(data, max_k=40):\n",
    "    # Lista para armazenar os valores de inÃ©rcia\n",
    "    inertia = []\n",
    "    \n",
    "    # Testar diferentes valores de k\n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plotar o grÃ¡fico do mÃ©todo do cotovelo\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(2, max_k + 1), inertia, marker='o')\n",
    "    plt.title('MÃ©todo do Cotovelo')\n",
    "    plt.xlabel('NÃºmero de Clusters (k)')\n",
    "    plt.ylabel('InÃ©rcia')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Chamar a funÃ§Ã£o para encontrar o nÃºmero ideal de clusters\n",
    "find_optimal_k(embeddings, max_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazer um kmeneans para encontrar o nÃºmero ideal de clusters utilizando o mÃ©todo da estatistica de Gap\n",
    "# O mÃ©todo da estatÃ­stica de Gap Ã© uma tÃ©cnica usada para determinar o nÃºmero ideal de clusters em um conjunto de dados\n",
    "\n",
    "def gap_statistic(data, n_clusters_range, n_repeats=10):\n",
    "    # Lista para armazenar os valores de Gap\n",
    "    gaps = []\n",
    "    \n",
    "    # Calcular o Gap para cada valor de k\n",
    "    for k in n_clusters_range:\n",
    "        # Ajustar o modelo KMeans\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        \n",
    "        # Calcular a inÃ©rcia do modelo ajustado\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        # Gerar dados aleatÃ³rios para comparaÃ§Ã£o\n",
    "        random_data = np.random.random_sample(size=data.shape)\n",
    "        \n",
    "        # Ajustar o modelo KMeans aos dados aleatÃ³rios\n",
    "        kmeans_random = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans_random.fit(random_data)\n",
    "        \n",
    "        # Calcular a inÃ©rcia do modelo ajustado aos dados aleatÃ³rios\n",
    "        inertia_random = kmeans_random.inertia_\n",
    "        \n",
    "        # Calcular o Gap e adicionar Ã  lista\n",
    "        gap = np.log(inertia_random) - np.log(inertia)\n",
    "        gaps.append(gap)\n",
    "    \n",
    "    return gaps\n",
    "\n",
    "# Definir o intervalo de k para testar\n",
    "n_clusters_range = range(30, 100)\n",
    "# Calcular a estatÃ­stica de Gap\n",
    "gaps = gap_statistic(embeddings, n_clusters_range)\n",
    "# Plotar os resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_clusters_range, gaps, marker='o')\n",
    "plt.title('EstatÃ­stica de Gap')\n",
    "plt.xlabel('NÃºmero de Clusters (k)')\n",
    "plt.ylabel('Gap')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d3830",
   "metadata": {},
   "source": [
    "Considerando que nos dois mÃ©todos que utilizamos de determinaÃ§Ã£o do numero otimo para o k deram inconclusivos vamos partir de uma hipotese a priori que toma em conta que na UFABC existem 31 cursos, sendo 4 BI/LI e 27 pÃ³s-BI/LI. Alem disso para cada curso temos disciplinas obrigatÃ³rias e limitadas. Assim vamos considerar a k=62 tendo em conta esses pontos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee47c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo o modelo de Kmeans para o numero de clusters determinado\n",
    "kmeans_final = KMeans(n_clusters=62, random_state=42, n_init=10)\n",
    "\n",
    "# Ajustar o modelo KMeans aos dados de embeddings\n",
    "# Encontrar disciplina que estÃ¡ no centroide de cada cluster\n",
    "kmeans_final.fit(embeddings)\n",
    "# Obter os rÃ³tulos dos clusters\n",
    "labels = kmeans_final.labels_\n",
    "# Obter os centrÃ³ides dos clusters\n",
    "centroids = kmeans_final.cluster_centers_\n",
    "# Calcular a distÃ¢ncia euclidiana entre os embeddings e os centrÃ³ides\n",
    "distances = euclidean_distances(embeddings, centroids)\n",
    "# Obter o Ã­ndice do centrÃ³ide mais prÃ³ximo para cada disciplina\n",
    "closest_centroid_indices = np.argmin(distances, axis=0)\n",
    "# printar os rÃ³tulos dos clusters e os centrÃ³ides\n",
    "for i in range(len(centroids)):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    # Obter os Ã­ndices das disciplinas que pertencem ao cluster i\n",
    "    cluster_indices = np.where(labels == i)[0]\n",
    "    # Obter os nomes das disciplinas que pertencem ao cluster i\n",
    "    cluster_disciplines = df.iloc[cluster_indices]['DISCIPLINA'].tolist()\n",
    "    # Obter o nome da disciplina mais prÃ³xima do centrÃ³ide\n",
    "    closest_discipline = df.iloc[closest_centroid_indices[i]]['DISCIPLINA']\n",
    "    print(f\"Disciplinas: {cluster_disciplines}\")\n",
    "    print(f\"Disciplina mais prÃ³xima do centrÃ³ide: {closest_discipline}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f7907",
   "metadata": {},
   "source": [
    "## 8. CÃ¡lculo de Score Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara caracteristicas\n",
    "def prepare_final_features(pairs, tpei_dif, prereq_sim, ementa_sim):\n",
    "    features = []\n",
    "    for pair in pairs:\n",
    "        prereq = prereq_sim.get(pair, 0.0)\n",
    "        ementa = ementa_sim(pair, 0.0)\n",
    "        features.append([tpei_dif, prereq, ementa])\n",
    "    return np.array(features)\n",
    "\n",
    "# Treino SVM com Kernel RBF\n",
    "# Codigo da Larissa\n",
    "\n",
    "# Calcula limiar Ã³timo\n",
    "y_prob = svm_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_final, y_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "final_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "all_final_features = prepare_final_features(\n",
    "    filtered_pairs, tpei_dif, prereq_sim, ementa_sim\n",
    ")\n",
    "all_scaled = scaler.transform(all_final_features)\n",
    "final_scores = svm_model.predict_proba(all_scaled)[:, 1]\n",
    "\n",
    "#Lista final de pares\n",
    "equivalent_pairs = [\n",
    "    (pair, score) for pair, score in zip(filtered_pairs, final_scores)\n",
    "    if score >= final_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff0ccc",
   "metadata": {},
   "source": [
    "## 9. ExplicaÃ§Ã£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "explainer = shap.KernelExplainer(\n",
    "    svm_model.predict_proba,\n",
    "    X_scaled,\n",
    "    link=\"logit\"\n",
    ")\n",
    "\n",
    "shap_values = explainer.shap_values(all_scaled)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "feature_names = ['DiferenÃ§a de crÃ©ditos', 'Similaridade de prÃ©-requisito', 'Similaridade de ementa']\n",
    "importance = np.abs(shap_values).mean(0)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "def explain_prediction(pair_index, features, shap_values):\n",
    "    feature_values = features[pair_index]\n",
    "    shap_value = shap_values[pair_index]\n",
    "    \n",
    "    explanation = []\n",
    "    for i, (name, value, impact) in enumerate(zip(feature_names, feature_values, shap_value)):\n",
    "        direction = \"positive\" if impact > 0 else \"negative\"\n",
    "        explanation.append(f\"{name}: {value:.3f} ({direction} impact: {abs(impact):.3f})\")\n",
    "    \n",
    "    return \"\\n\".join(explanation)\n",
    "\n",
    "# Generate explanations for all equivalent pairs\n",
    "explanations = []\n",
    "for i, (pair, score) in enumerate(equivalent_pairs):\n",
    "    explanation = f\"Pair: {pair[0]} - {pair[1]}\\n\"\n",
    "    explanation += f\"Equivalence Score: {score:.3f}\\n\"\n",
    "    explanation += \"Feature Contributions:\\n\"\n",
    "    explanation += explain_prediction(i, all_final_features, shap_values)\n",
    "    explanations.append(explanation)\n",
    "# Initialize SHAP explainer for SVM model\n",
    "# Calculate SHAP values for each prediction\n",
    "# Generate feature importance rankings\n",
    "# Create individual prediction explanations\n",
    "# Prepare text explanations for results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79476c28",
   "metadata": {},
   "source": [
    "## 10. VisualizaÃ§Ãµes dos Resultados\n",
    "\n",
    "### 10.1 Matriz de visualizaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "df = pd.DataFrame\n",
    "\n",
    "# Adicionar nÃ³s (disciplinas)\n",
    "for sigla in df['DISCIPLINA'].unique():\n",
    "    G.add_node(sigla)\n",
    "\n",
    "# Modificar a criaÃ§Ã£o de arestas\n",
    "for pair in similar_pairs:\n",
    "    disciplina_a, disciplina_b, similarity = pair\n",
    "    if similarity >= 0.8:  # Ajuste o threshold aqui\n",
    "        G.add_edge(disciplina_a, disciplina_b, weight=similarity)\n",
    "\n",
    "        # Calcular posiÃ§Ãµes dos nÃ³s\n",
    "        pos = nx.kamada_kawai_layout(G, weight='weight')  # Usa o peso (similaridade) para organizar\n",
    "\n",
    "        # Criar traÃ§os para arestas\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])  # None para separar linhas\n",
    "            edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line= dict(width=1, color='#888'),  # Espessura e cor das arestas\n",
    "            hoverinfo='none',\n",
    "            mode='lines')\n",
    "\n",
    "# Criar traÃ§os para nÃ³s\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_text = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)  # Texto ao passar o mouse\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "  x=node_x, y=node_y,\n",
    "  mode='markers+text',\n",
    "  text=node_text,\n",
    "  textposition=\"top center\",\n",
    "  hoverinfo='text',\n",
    "  marker=dict(\n",
    "      showscale=True,\n",
    "      colorscale='YlGnBu',\n",
    "      size=15,\n",
    "      color=[],  # Pode ser usado para codificar cores por comunidade\n",
    "      line=dict(width=2, color='black'))\n",
    ")\n",
    "\n",
    "# Criar figura\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "    layout=go.Layout(\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=0, l=0, r=0, t=0),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    ")\n",
    "\n",
    "# Adicionar interatividade (exibir sigla ao passar o mouse)\n",
    "fig.update_traces(textposition='top center', hoverinfo='text')\n",
    "\n",
    "from networkx.algorithms import community\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "# Atribuir cores diferentes a cada comunidade\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create DiGraph visualization showing prerequisite relationships\n",
    "# Color nodes based on equivalence status\n",
    "# Highlight connected discipline pairs\n",
    "# Add node labels and edge weights\n",
    "# Save graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d967",
   "metadata": {},
   "source": [
    "### 10.2 VisualizaÃ§Ã£o por mapa de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79b1aed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'similaridades_disciplinas_filtrado.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Carrega o TSV\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33msimilaridades_disciplinas_filtrado.tsv\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Seleciona 50 pares representativos\u001b[39;00m\n\u001b[32m      9\u001b[39m amostras = pd.concat([\n\u001b[32m     10\u001b[39m     df.nlargest(\u001b[32m10\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscore_combinado\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     11\u001b[39m     df.nsmallest(\u001b[32m10\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscore_combinado\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     df.sample(\u001b[32m10\u001b[39m, random_state=\u001b[32m7\u001b[39m)\n\u001b[32m     15\u001b[39m ]).drop_duplicates().reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = TextFileReader(filepath_or_buffer, **kwds)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28mself\u001b[39m._make_engine(f, \u001b[38;5;28mself\u001b[39m.engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = get_handle(\n\u001b[32m   1881\u001b[39m     f,\n\u001b[32m   1882\u001b[39m     mode,\n\u001b[32m   1883\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1884\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1885\u001b[39m     memory_map=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mmemory_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1886\u001b[39m     is_text=is_text,\n\u001b[32m   1887\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mencoding_errors\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1888\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1889\u001b[39m )\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'similaridades_disciplinas_filtrado.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carrega o TSV\n",
    "df = pd.read_csv(\"similaridades_disciplinas_filtrado.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Seleciona 50 pares representativos\n",
    "amostras = pd.concat([\n",
    "    df.nlargest(10, 'score_combinado'),\n",
    "    df.nsmallest(10, 'score_combinado'),\n",
    "    df.nlargest(10, 'node2vec'),\n",
    "    df[df['jaccard_pred'] > 0].sample(10, random_state=42),\n",
    "    df.sample(10, random_state=7)\n",
    "]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# IndexaÃ§Ã£o legÃ­vel\n",
    "amostras.index = amostras[\"disciplina_a\"] + \" vs \" + amostras[\"disciplina_b\"]\n",
    "\n",
    "# MÃ©tricas para o heatmap\n",
    "metricas = [\"score_combinado\", \"jaccard_pred\", \"jaccard_succ\", \"profundidade\", \"node2vec\"]\n",
    "\n",
    "# Cria o heatmap\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(amostras[metricas], annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "plt.title(\"Mapa de Calor: Similaridade Entre Disciplinas (50 pares)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd844e83",
   "metadata": {},
   "source": [
    "O grÃ¡fico abaixo apresenta um heatmap com 50 pares de disciplinas selecionados de forma representativa para demonstrar padrÃµes nas mÃ©tricas de similaridade utilizadas:\n",
    "\n",
    "score_combinado: medida geral de similaridade calculada com base em mÃºltiplos critÃ©rios.\n",
    "\n",
    "jaccard_pred e jaccard_succ: medem sobreposiÃ§Ã£o de prÃ©-requisitos e sucessores, respectivamente.\n",
    "\n",
    "profundidade: indica o quÃ£o prÃ³ximas as disciplinas estÃ£o em termos de nÃ­vel curricular.\n",
    "\n",
    "node2vec: representa a semelhanÃ§a estrutural entre disciplinas no grafo de currÃ­culo.\n",
    "\n",
    "Os pares foram escolhidos para cobrir casos com alta e baixa similaridade, fortes conexÃµes por embeddings, e exemplos aleatÃ³rios, oferecendo interpretÃ¡vel das relaÃ§Ãµes entre disciplinas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce9bc0",
   "metadata": {},
   "source": [
    "![alt text](mapac-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcaaf6",
   "metadata": {},
   "source": [
    "### 10.3 Matrizes de ConfusÃ£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbad8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for model performance\n",
    "# Create heatmap visualization of confusion matrix\n",
    "# Add precision, recall, and F1 scores\n",
    "# Generate performance metrics report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868a7f5",
   "metadata": {},
   "source": [
    "## 11. ExportaÃ§Ã£o de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f65c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered results to TSV file\n",
    "# Export model performance metrics\n",
    "# Save visualizations in specified formats\n",
    "# Generate final summary report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_disciplinas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
