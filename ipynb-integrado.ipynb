{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343744a0",
   "metadata": {},
   "source": [
    "# Sistema de Detec√ß√£o de Equival√™ncia de Disciplinas\n",
    "\n",
    "# Notebook de verifica√ß√£o da similaridade entre ementas de disciplinas\n",
    "\n",
    "## Introdu√ß√£o:\n",
    "O estudante da UFABC passa muito tempo j√° em sua gradua√ß√£o para terminar as disciplinas do seu BI e do p√≥s-BI e isso afeta principalmente os alunos de cursos mais concorridos como √© o de computa√ß√£o, em que as disciplinas podem ter mais de 150% de requisi√ß√£o. Tendo em vista isso, na UFABC temos dois processos j√° estruturados que √© o processo de covalida√ß√£o e de equival√™ncia, normatizados nas resolu√ß√µes ConsEPE n¬∫ 157/2013 e CG N¬∫ 023/2019 respectivamente. Covalida√ß√£o √© um processo interno da UFABC que √© basicamente para a transi√ß√£o de projetos pedag√≥gicos, de forma que o(a) estudante consegue integralizar o curso em um PPC antigo com disciplinas novas e a equival√™ncia √© um processo que uma disciplina de fora pode ter alguma similaridade de uma disciplina da ufabc e do curso que voc√™ quer se formar. Assim est√° presente na Resolu√ß√£o CG N¬∫ 023/2019 o seguinte:\n",
    "\n",
    ">Art. 4¬∫ Consistem em requisitos para a dispensa por equival√™ncia, para disciplinas\n",
    "cursadas no Brasil:\n",
    "\n",
    ">> I. a carga hor√°ria total da disciplina cursada deve ser igual ou maior √† carga hor√°ria da que se pede equival√™ncia;\n",
    "\n",
    ">> II. o conte√∫do da disciplina cursada deve ser compat√≠vel e correspondente a, no m√≠nimo, 75% (setenta e cinco por cento) do conte√∫do daquela de que se pede equival√™ncia, considerando-se teoria e pr√°tica, quando for o caso. \n",
    "\n",
    ">>>Par√°grafo √∫nico: Excepcionalmente, e mediante justificativa, a coordena√ß√£o de curso pode autorizar equival√™ncias que cumpram parcialmente estes requisito\n",
    "\n",
    "Utilizando essa base das normativas e os cat√°logos de disciplinas da UFABC, objetivamos gerar um sistema de pr√©-avalia√ß√£o de disciplinas com alta chance de convalida√ß√£o, reduzindo o espa√ßo de busca dos t√©cnicos responsaveis pela aprova√ß√£o de pares de disciplinas com convalida√ß√£o e garantindo a inclus√£o de todas as disciplinas na an√°lise. Os benef√≠cio desta proposta s√£o a economia de recursos, maior integraliza√ß√£o de diferentes PPCs e promove efetivamente a interdisciplinaridade, fundamento da UFAB, uma vez que os cursos poderiam ofertar a mesma disciplina com diferentes enfoques no mesmo quadrimestre, melhorando a qualidade do ensino. Os recursos poupados s√£o tanto na carga de trabalho dos t√©cnicos quanto recursos computacionais (realizada manualmente, ess tarefa tem um complexidade O(n^2)), requerindo an√°lise manual apenas das disciplinas que se encaixam nos crit√©rios das resolu√ß√µes ConsEPE n¬∫ 157/2013 e CG N¬∫ 023/2019 e com alta probabilidade de valida√ß√£o. \n",
    "\n",
    "Dessa forma propomos uma an√°lise de similaridade semantica e relacional entre as ementas, os pr√©-requisitos das disciplinas da UFABC e dos valores de TPEI, para fim de pr√©-avalia√ß√£o de equival√™ncias entre as diciplinas da universidade cumprindo com o Art. 4¬∞ da resolu√ß√£o CG N¬∫ 023/2019. Assim. em disciplinas que verificarmos que existe uma similaridade equivalente a de pares de disciplinas atualmente validadas (similaridade maior ou igual a 75% e que cumpre a quantidade de creditos da outra disciplina), poderemos gerar listas de pares para aprova√ß√£o manual em ordem de similaridde. O benef√≠cio proporcionado pela maior integra√ß√£o do sistema √© dif√≠cil de mensurar mas, al√©m de afetar todes es envolvides no processo discente, o alinhamento dessa solu√ß√£o com os sistemas vigentes da universidade amplificam seu impacto e condi√ß√µes em que ele se manifesta.\n",
    "\n",
    "## Objetivos\n",
    "1. Fazer uma proposta de equival√™ncia interna de disciplinas a partir da an√°lise de similaridade sem√¢ntica entre as ementas das disciplinas da UFABC da Gradua√ß√£o e da quantidade de TPEI que elas tem\n",
    "2. Reduzir o espa√ßo de busca de O(n¬≤) para aproximadamente O(n*log(n)) a partir da aplica√ß√£o de filtros de TPEI para quantidade de cr√©ditos das disciplinas, de forma s√≥ comparar as diciplinas que tem a quantidade de cr√©ditos iguais\n",
    "\n",
    "## M√©todos\n",
    "1. Embedding sem√¢ntico do conte√∫do dos Objetivos e Ementas das disciplinas utilizando BERTimbau\n",
    "2. C√°lculo de similaridade cosseno para similaridade entre os textos \n",
    "3. Kmeans nos embeddings para determinar agrupamentos de disciplinas ajudando no processo de equival√™ncia\n",
    "4. Grafo relacional dos Pr√©-Requisitos entre disciplinas\n",
    "5. C√°lculo de similaridade utiliando dist√¢ncias de jaccard\n",
    "6. Otimiza√ß√£o dos hiperpar√¢metros dos filtros de redu√ß√£o do espa√ßo de busca utilizando modelos de √°rvores aleat√≥rias \n",
    "\n",
    "## 1. Imports e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza imports necess√°rios\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import unicodedata\n",
    "import requests\n",
    "from io import StringIO\n",
    "from node2vec import Node2Vec\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85da40",
   "metadata": {},
   "source": [
    "## 2. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f4b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixa o cat√°logo da UFABC direto do Reposit√≥rio no GitHub\n",
    "def carregar_catalogo():\n",
    "    url = \"https://raw.githubusercontent.com/angeloodr/disciplinas-ufabc/main/catalogo_disciplinas_graduacao_2024_2025.tsv\"\n",
    "    print(\"üîÑ Baixando cat√°logo de disciplinas...\")\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    df = pd.read_csv(StringIO(resp.text), sep='\\t')\n",
    "\n",
    "    # Normaliza os nomes das colunas\n",
    "    df.columns = [\n",
    "        normalize_str(col).upper().replace(' ', '_')\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    print(\"‚úÖ Download bem-sucedido!\")\n",
    "    print(\"üìù Colunas dispon√≠veis:\", df.columns.tolist())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7a644",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    norm = unicodedata.normalize('NFKD', str(text))\n",
    "    norm = norm.encode('ASCII', 'ignore').decode('utf-8')\n",
    "    norm = re.sub(r'[^\\w\\s]', '')\n",
    "    norm = norm.lower().strip()\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def extract_tpei(tpei_str):\n",
    "    if pd.isna(tpei_str):\n",
    "        return []\n",
    "    \n",
    "    values = tpei_str.split('-')\n",
    "    return {\n",
    "        'teoria': int(values[0]),\n",
    "        'pratica': int(values[1]),\n",
    "        'extensao': int(values[2]),\n",
    "        'individual': int(values[3]),\n",
    "        'total_creditos': int(values[0]) + int(values[1])  # T+P only\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def extract_prereq(recomendacao):\n",
    "    if pd.isna(recomendacao) or recomendacao.strip() == '':\n",
    "        return []\n",
    "\n",
    "    prereqs = []\n",
    "    for part in recomendacao.split(';'):\n",
    "        part = part.strip()\n",
    "        if part:\n",
    "            prereqs.append(part)\n",
    "    return prereqs\n",
    "\n",
    "def extract_cod(prereqs):\n",
    "    if pd.isna(prereqs):\n",
    "        return []\n",
    "    for sigla in prereqs.split(';'):\n",
    "        sigla = sigla.strip()\n",
    "        if sigla:\n",
    "            prereqs.append(sigla)\n",
    "    return prereqs\n",
    "\n",
    "def create_allfeats(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # TPEI\n",
    "    tpei_feats = df['TPEI'].apply(extract_tpei)\n",
    "    df['teoria'] = tpei_feats.apply(lambda x: x['teoria'])\n",
    "    df['pratica'] = tpei_feats.apply(lambda x: x['pratica'])\n",
    "    df['extensao'] = tpei_feats.apply(lambda x: x['extensao'])\n",
    "    df['individual'] = tpei_feats.apply(lambda x: x['individual'])\n",
    "    df['total_creditos'] = tpei_feats.apply(lambda x: x['total_creditos'])\n",
    "\n",
    "    # Ementa\n",
    "    df['ementa_norm'] = df['EMENTA'].apply(normalize_text)\n",
    "    df['objetivos_norm'] = df['OBJETIVOS'].apply(normalize_text)\n",
    "\n",
    "    # Pr√©-requisitos\n",
    "    df['prerequisites'] = df['RECOMENDA√á√ÉO'].apply(extract_prereq)\n",
    "    df['num_prerequisites'] = df['prerequisites'].apply(len)\n",
    "    df['codigo'] = df['SIGLA'].apply(extract_cod)\n",
    "\n",
    "    return df\n",
    "\n",
    "def sao_variantes_simples(nome1, nome2):\n",
    "    base1 = re.sub(r'[\\s\\-]*(laborat√≥rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome1.strip(), flags=re.IGNORECASE)\n",
    "    base2 = re.sub(r'[\\s\\-]*(laborat√≥rio|[a-zA-Z]{1,3}|[ivxIVX0-9]{1,4})\\s*$', '', nome2.strip(), flags=re.IGNORECASE)\n",
    "    return base1.lower() == base2.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd40eb",
   "metadata": {},
   "source": [
    "## 4. Filtro TPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eab5d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_filtro_tpei(df_pares, df_disciplinas):\n",
    "    print(\"üîÑ Aplicando filtro TPEI...\")\n",
    "    \n",
    "    # Criar dicion√°rio para mapeamento r√°pido de sigla -> cr√©ditos\n",
    "    creditos_dict = {row['SIGLA']: row['total_creditos'] \n",
    "                    for _, row in df_disciplinas.iterrows() \n",
    "                    if pd.notna(row['SIGLA'])}\n",
    "    \n",
    "    # DataFrame para armazenar pares filtrados\n",
    "    pares_filtrados = []\n",
    "    \n",
    "    # Dicion√°rio para armazenar diferen√ßas de cr√©ditos\n",
    "    tpei_dif = {}\n",
    "    \n",
    "    # Registrar pares exclu√≠dos para log\n",
    "    pares_excluidos = []\n",
    "    \n",
    "    # Aplicar filtro a cada par\n",
    "    for _, row in df_pares.iterrows():\n",
    "        sigla_a = row['SIGLA_A']  # current\n",
    "        sigla_b = row['SIGLA_B']  # comp\n",
    "        \n",
    "        # Verificar se ambas as siglas existem no dicion√°rio\n",
    "        if sigla_a not in creditos_dict or sigla_b not in creditos_dict:\n",
    "            pares_excluidos.append((sigla_a, sigla_b, \"Sigla n√£o encontrada\"))\n",
    "            continue\n",
    "        \n",
    "        creditos_a = creditos_dict[sigla_a]\n",
    "        creditos_b = creditos_dict[sigla_b]\n",
    "        \n",
    "        # Verificar a regra de filtro\n",
    "        if creditos_a > creditos_b:\n",
    "            # Remover par se cr√©ditos de A > cr√©ditos de B\n",
    "            pares_excluidos.append((sigla_a, sigla_b, f\"Cr√©ditos A({creditos_a}) > B({creditos_b})\"))\n",
    "        else:\n",
    "            # Manter o par e calcular a diferen√ßa\n",
    "            pares_filtrados.append(row)\n",
    "            tpei_dif[(sigla_a, sigla_b)] = creditos_b - creditos_a\n",
    "    \n",
    "    # Criar DataFrame com pares filtrados\n",
    "    df_filtrado = pd.DataFrame(pares_filtrados)\n",
    "    \n",
    "    print(f\"‚úÖ Filtro TPEI aplicado: {len(df_filtrado)} pares mantidos, {len(pares_excluidos)} exclu√≠dos\")\n",
    "    \n",
    "    return df_filtrado, tpei_dif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e34318",
   "metadata": {},
   "source": [
    "## 5. Filtro de Pr√©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DiGraph from prerequisite relationships\n",
    "# Generate node2vec embeddings for disciplines\n",
    "# Calculate cosine distance between discipline embeddings\n",
    "# Apply prerequisite similarity threshold\n",
    "# Filter pairs based on prerequisite similarity\n",
    "\n",
    "#  Constr√≥i grafo com arestas de pr√©-requisito com base na coluna RECOMENDACAO\n",
    "def construir_grafo(df: pd.DataFrame) -> nx.DiGraph:\n",
    "    print(\"üìå Construindo grafo de pr√©-requisitos...\")\n",
    "    G = nx.DiGraph()\n",
    "    total_arestas = 0\n",
    "\n",
    "    # Cria um dicion√°rio de nome normalizado ‚Üí sigla\n",
    "    mapping = {\n",
    "        normalize_text(row['DISCIPLINA']): row['SIGLA']\n",
    "        for _, row in df.iterrows()\n",
    "        if pd.notna(row['SIGLA'])\n",
    "    }\n",
    "\n",
    "    # Adiciona todos os n√≥s com metadados\n",
    "    for _, row in df.iterrows():\n",
    "        sigla = row['SIGLA']\n",
    "        nome = row['DISCIPLINA']\n",
    "        if pd.isna(sigla): \n",
    "            continue\n",
    "        G.add_node(sigla, nome=nome)\n",
    "\n",
    "    # Adiciona as arestas baseadas nas recomenda√ß√µes\n",
    "    for _, row in df.iterrows():\n",
    "        curso = row['SIGLA']\n",
    "        if pd.isna(curso):\n",
    "            continue\n",
    "\n",
    "        recs = row.get('RECOMENDACAO', '')\n",
    "        if pd.isna(recs) or not isinstance(recs, str):\n",
    "            continue\n",
    "\n",
    "        for rec in recs.split(';'):\n",
    "            rec = rec.strip()\n",
    "            if not rec:\n",
    "                continue\n",
    "            rec_norm = normalize_text(rec)\n",
    "            if rec_norm in mapping:\n",
    "                prereq = mapping[rec_norm]\n",
    "                if not G.has_edge(prereq, curso):\n",
    "                    G.add_edge(prereq, curso, tipo='pre_requisito')\n",
    "                    total_arestas += 1\n",
    "\n",
    "    print(f\"‚úÖ Grafo criado com {G.number_of_nodes()} n√≥s e {total_arestas} arestas.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc68d6",
   "metadata": {},
   "source": [
    "### Processamento do Grafo: Rede Direcionada e Embeddings Estruturais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remove ciclos do grafo para que seja um DAG (necess√°rio para c√°lculo de profundidade)\n",
    "def remover_ciclos(G: nx.DiGraph) -> nx.DiGraph:\n",
    "    print(\"üîÅ Removendo ciclos (modo eficiente)...\")\n",
    "    G_copy = G.copy()\n",
    "    removidos = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ciclo = nx.find_cycle(G_copy, orientation='original')\n",
    "            if ciclo:\n",
    "                # Remove a primeira aresta do ciclo\n",
    "                u, v, _ = ciclo[0]\n",
    "                G_copy.remove_edge(u, v)\n",
    "                removidos += 1\n",
    "    except nx.NetworkXNoCycle:\n",
    "        pass  # N√£o h√° mais ciclos\n",
    "\n",
    "    print(f\"‚úÖ Ciclos removidos: {removidos}\")\n",
    "    return G_copy\n",
    "\n",
    "# Gera embeddings estruturais com Node2Vec\n",
    "def gerar_embeddings_node2vec(G: nx.DiGraph, dimensions=64, walk_length=10, num_walks=50, workers=1):\n",
    "    print(\"üîÑ Gerando embeddings estruturais (Node2Vec)...\")\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers)\n",
    "    model = node2vec.fit(window=5, min_count=1, batch_words=4)\n",
    "    embeddings = {node: model.wv[node] for node in G.nodes()}\n",
    "    print(\"‚úÖ Embeddings gerados!\")\n",
    "    return embeddings\n",
    "\n",
    "# Calcula profundidade de cada n√≥ no grafo (camada curricular)\n",
    "def calcular_profundidade(G: nx.DiGraph):\n",
    "    print(\"üìè Calculando profundidade no grafo (DAG)...\")\n",
    "    profundidades = {}\n",
    "    for node in nx.topological_sort(G):\n",
    "        preds = list(G.predecessors(node))\n",
    "        if not preds:\n",
    "            profundidades[node] = 0\n",
    "        else:\n",
    "            profundidades[node] = max(profundidades[p] for p in preds) + 1\n",
    "    print(\"‚úÖ Profundidade calculada!\")\n",
    "    return profundidades\n",
    "\n",
    "# Salva os embeddings em CSV\n",
    "def salvar_embeddings_csv(embeddings, caminho=\"embeddings_node2vec.csv\"):\n",
    "    df = pd.DataFrame.from_dict(embeddings, orient='index')\n",
    "    df.index.name = 'DISCIPLINA'\n",
    "    df.to_csv(caminho)\n",
    "    print(f\"üíæ Embeddings salvos em: {caminho}\")\n",
    "\n",
    "# Fluxo principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Etapa 1 - Carregar cat√°logo\n",
    "    df = carregar_catalogo()\n",
    "\n",
    "    # Etapa 2 - Construir grafo\n",
    "    grafo = construir_grafo(df)\n",
    "    nx.write_graphml(grafo, \"grafo_pre_requisitos.graphml\")\n",
    "    print(\"üìÅ Arquivo salvo: grafo_pre_requisitos.graphml\")\n",
    "\n",
    "    # Etapa 3 - Gerar embeddings estruturais\n",
    "    embeddings = gerar_embeddings_node2vec(grafo)\n",
    "    salvar_embeddings_csv(embeddings)\n",
    "\n",
    "    # Etapa 4 - Remover ciclos e calcular profundidade\n",
    "    grafo_sem_ciclos = remover_ciclos(grafo)\n",
    "    profundidades = calcular_profundidade(grafo_sem_ciclos)\n",
    "\n",
    "    # Etapa 5 - Salvar profundidades em TXT\n",
    "    with open(\"profundidade_nos.txt\", \"w\") as f:\n",
    "        for node, prof in profundidades.items():\n",
    "            f.write(f\"{node}: {prof}\\n\")\n",
    "    print(\"üìÅ Profundidades salvas em: profundidade_nos.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0352f6",
   "metadata": {},
   "source": [
    "### Com o grafo pronto e os dados carregados, aplicaremos o √çndice de Jaccard e outras medidas de similaridade para analisar as conex√µes entre as disciplinas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b161e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard entre dois conjuntos\n",
    "def jaccard_similarity(set1, set2):\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "# Aplica Jaccard entre predecessores ou sucessores\n",
    "def similaridade_jaccard(G, a, b, tipo='predecessor'):\n",
    "    try:\n",
    "        if tipo == 'predecessor':\n",
    "            set_a = set(G.predecessors(a))\n",
    "            set_b = set(G.predecessors(b))\n",
    "        else:\n",
    "            set_a = set(G.successors(a))\n",
    "            set_b = set(G.successors(b))\n",
    "        return jaccard_similarity(set_a, set_b)\n",
    "    except nx.NetworkXError:\n",
    "        return 0.0\n",
    "    \n",
    "# Similaridade de profundidade\n",
    "def similaridade_profundidade(prof, a, b):\n",
    "    if a not in prof or b not in prof:\n",
    "        return 0.0\n",
    "    max_p = max(prof.values())\n",
    "    if max_p == 0:\n",
    "        return 1.0\n",
    "    return 1.0 - abs(prof[a] - prof[b]) / max_p\n",
    "\n",
    "# Similaridade por Node2Vec (cosseno)\n",
    "def similaridade_node2vec(embeddings, a, b):\n",
    "    if a not in embeddings.index or b not in embeddings.index:\n",
    "        return 0.0\n",
    "    vec_a = embeddings.loc[a].values.reshape(1, -1)\n",
    "    vec_b = embeddings.loc[b].values.reshape(1, -1)\n",
    "    return cosine_similarity(vec_a, vec_b)[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a62330",
   "metadata": {},
   "source": [
    "### Combina√ß√£o das Similaridades em um Score Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6325b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina m√∫ltiplas similaridades em score final\n",
    "def similaridade_combinada(G, embeddings, profundidades, a, b, pesos=None):\n",
    "    if pesos is None:\n",
    "        pesos = {\n",
    "            'jaccard_pred': 1.0,\n",
    "            'jaccard_succ': 1.0,\n",
    "            'profundidade': 1.0,\n",
    "            'node2vec': 1.0,\n",
    "        }\n",
    "\n",
    "    sim_jaccard_pred = similaridade_jaccard(G, a, b, tipo='predecessor')\n",
    "    sim_jaccard_succ = similaridade_jaccard(G, a, b, tipo='successor')\n",
    "    sim_profundidade = similaridade_profundidade(profundidades, a, b)\n",
    "    sim_node2vec = similaridade_node2vec(embeddings, a, b)\n",
    "\n",
    "    total_peso = sum(pesos.values())\n",
    "    score = (\n",
    "        pesos['jaccard_pred'] * sim_jaccard_pred +\n",
    "        pesos['jaccard_succ'] * sim_jaccard_succ +\n",
    "        pesos['profundidade'] * sim_profundidade +\n",
    "        pesos['node2vec'] * sim_node2vec\n",
    "    ) / total_peso\n",
    "\n",
    "    return {\n",
    "        \"score_combinado\": score,\n",
    "        \"jaccard_pred\": sim_jaccard_pred,\n",
    "        \"jaccard_succ\": sim_jaccard_succ,\n",
    "        \"profundidade\": sim_profundidade,\n",
    "        \"node2vec\": sim_node2vec\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b286240",
   "metadata": {},
   "source": [
    "###  Gera√ß√£o do arquivo de similaridades filtradas (.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d611736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrita incremental com filtro de profundidade e TPEI\n",
    "if __name__ == \"__main__\":\n",
    "    grafo = carregar_grafo()\n",
    "    embeddings = carregar_embeddings()\n",
    "    profundidades = carregar_profundidades()\n",
    "\n",
    "    caminho_saida = \"similaridades_disciplinas_filtrado.tsv\"\n",
    "    with open(caminho_saida, mode=\"w\", newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out, delimiter=\"\\t\")\n",
    "        # Cabe√ßalho\n",
    "        writer.writerow([\n",
    "            \"disciplina_a\", \"disciplina_b\", \"score_combinado\",\n",
    "            \"jaccard_pred\", \"jaccard_succ\", \"profundidade\", \"node2vec\"\n",
    "        ])\n",
    "\n",
    "        # La√ßo otimizado com filtros\n",
    "        for a in grafo.nodes():\n",
    "            for b in grafo.nodes():\n",
    "                if a == b:\n",
    "                    continue\n",
    "\n",
    "                # Filtro por profundidade (ex: 2 n√≠veis no m√°ximo)\n",
    "                if abs(profundidades.get(a, 0) - profundidades.get(b, 0)) > 2:\n",
    "                    continue\n",
    "\n",
    "                sim = similaridade_combinada(grafo, embeddings, profundidades, a, b)\n",
    "                writer.writerow([\n",
    "                    a, b, sim[\"score_combinado\"],\n",
    "                    sim[\"jaccard_pred\"], sim[\"jaccard_succ\"],\n",
    "                    sim[\"profundidade\"], sim[\"node2vec\"]\n",
    "                ])\n",
    "\n",
    "    print(f\"‚úÖ Arquivo '{caminho_saida}' gerado com sucesso.\")\n",
    "\n",
    "    # AVISO: O ARQUIVO .TSV CONTENDO O RESULTADO DAS AN√ÅLISES PODE DEMORAR PARA SER GERADO DEPENDENDO DA M√ÅQUINA UTILIZADA PARA A EXECU√á√ÉO!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc787716",
   "metadata": {},
   "source": [
    "## 6. C√°lculo de Score TPEI + Pr√©-requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a218d884",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_catboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(features)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Treina CatBoost usando labeled_data\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# C√≥digo da Larissa\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Otimiza limiar\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# from sklearn.metrics import precision_recall_curve\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m y_pred_proba = model_catboost.predict_proba(X_val)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     24\u001b[39m precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n\u001b[32m     25\u001b[39m f1_scores = \u001b[32m2\u001b[39m * (precision * recall) / (precision + recall + \u001b[32m1e-8\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_catboost' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepara matriz de caracteristicas\n",
    "def prepare_combined_features(discipline_pairs, tpei_diff, prereq_similarities):\n",
    "    features = []\n",
    "    \n",
    "    pares_filtrados = []\n",
    "    \n",
    "    pares_excluidos = []\n",
    "    \n",
    "    for pair in discipline_pairs:\n",
    "        prereq_sim = prereq_similarities.get(pairs, 0.0)\n",
    "        features.append([tpei_diff, prereq_sim])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Treina CatBoost usando labeled_data\n",
    "# C√≥digo da Larissa\n",
    "\n",
    "\n",
    "\n",
    "# Otimiza limiar\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "y_pred_proba = model_catboost.predict_proba(X_val)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Calcula score de cada par\n",
    "all_features = prepare_combined_features(candidate_pairs, tpei_diff, prereq_similarities)\n",
    "combined_scores = model_catboost.predict_proba(all_features)[:, 1]\n",
    "\n",
    "# Aplica filtro em cada par\n",
    "filtered_pairs = [\n",
    "    pair for pair, score in zip(candidate_pairs, combined_scores)\n",
    "    if score >= optimal_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5bf8a",
   "metadata": {},
   "source": [
    "## 7. Filtro de Ementa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3744b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "parameter without a default follows parameter with a default (505583953.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef similariry_between_DISCIPLINA(cosine_sim, similarity_threshold = 0.75, df):\u001b[39m\n                                                                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m parameter without a default follows parameter with a default\n"
     ]
    }
   ],
   "source": [
    "# Load BERTimbau pre-trained model\n",
    "# Generate embeddings for ementa texts\n",
    "# Apply node2vec to ementa embeddings\n",
    "# Calculate cosine distance between ementa embeddings\n",
    "# Filter based on semantic similarity threshold\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#busca N^2\n",
    "def similariry_between_DISCIPLINA(cosine_sim, similarity_threshold = 0.8):\n",
    "    # Encontrar pares com similaridade ‚â• 75%\n",
    "    similar_pairs = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):  # Evitar duplicatas (i, j) e (j, i)\n",
    "            if cosine_sim[i, j] >= similarity_threshold:\n",
    "                similar_pairs.append((df.iloc[i]['DISCIPLINA'], df.iloc[j]['DISCIPLINA'], cosine_sim[i, j]))\n",
    "\n",
    "    # Exibir resultados\n",
    "    for pair in similar_pairs:\n",
    "        print(f\"Disciplinas similares: {pair[0]} e {pair[1]} (Similaridade: {pair[2]:.2f})\")\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "model = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
    "embeddings = model.encode(df['EMENTA_PREPROCESSED'].tolist())\n",
    "\n",
    "cosine_sim_bert = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "sim_pair = similariry_between_DISCIPLINA(cosine_sim_bert)\n",
    "\n",
    "\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "# Criar modelo SBERT a partir do BERT\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode_mean_tokens=True)\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "embeddings = model.encode(pd.DataFrame['EMENTA'].tolist())\n",
    "cosine_sim_sbert = cosine_similarity(embeddings, embeddings)\n",
    "sim_ementa = similariry_between_DISCIPLINA(cosine_sim_sbert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f7907",
   "metadata": {},
   "source": [
    "## 8. C√°lculo de Score Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara caracteristicas\n",
    "def prepare_final_features(pairs, tpei_dif, prereq_sim, ementa_sim):\n",
    "    features = []\n",
    "    for pair in pairs:\n",
    "        prereq = prereq_sim.get(pair, 0.0)\n",
    "        ementa = ementa_sim(pair, 0.0)\n",
    "        features.append([tpei_dif, prereq, ementa])\n",
    "    return np.array(features)\n",
    "\n",
    "# Treino SVM com Kernel RBF\n",
    "# Codigo da Larissa\n",
    "\n",
    "# Calcula limiar √≥timo\n",
    "y_prob = svm_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_final, y_prob)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "final_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "all_final_features = prepare_final_features(\n",
    "    filtered_pairs, tpei_dif, prereq_sim, ementa_sim\n",
    ")\n",
    "all_scaled = scaler.transform(all_final_features)\n",
    "final_scores = svm_model.predict_proba(all_scaled)[:, 1]\n",
    "\n",
    "#Lista final de pares\n",
    "equivalent_pairs = [\n",
    "    (pair, score) for pair, score in zip(filtered_pairs, final_scores)\n",
    "    if score >= final_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff0ccc",
   "metadata": {},
   "source": [
    "## 9. Explica√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "explainer = shap.KernelExplainer(\n",
    "    svm_model.predict_proba,\n",
    "    X_scaled,\n",
    "    link=\"logit\"\n",
    ")\n",
    "\n",
    "shap_values = explainer.shap_values(all_scaled)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "feature_names = ['Diferen√ßa de cr√©ditos', 'Similaridade de pr√©-requisito', 'Similaridade de ementa']\n",
    "importance = np.abs(shap_values).mean(0)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "def explain_prediction(pair_index, features, shap_values):\n",
    "    feature_values = features[pair_index]\n",
    "    shap_value = shap_values[pair_index]\n",
    "    \n",
    "    explanation = []\n",
    "    for i, (name, value, impact) in enumerate(zip(feature_names, feature_values, shap_value)):\n",
    "        direction = \"positive\" if impact > 0 else \"negative\"\n",
    "        explanation.append(f\"{name}: {value:.3f} ({direction} impact: {abs(impact):.3f})\")\n",
    "    \n",
    "    return \"\\n\".join(explanation)\n",
    "\n",
    "# Generate explanations for all equivalent pairs\n",
    "explanations = []\n",
    "for i, (pair, score) in enumerate(equivalent_pairs):\n",
    "    explanation = f\"Pair: {pair[0]} - {pair[1]}\\n\"\n",
    "    explanation += f\"Equivalence Score: {score:.3f}\\n\"\n",
    "    explanation += \"Feature Contributions:\\n\"\n",
    "    explanation += explain_prediction(i, all_final_features, shap_values)\n",
    "    explanations.append(explanation)\n",
    "# Initialize SHAP explainer for SVM model\n",
    "# Calculate SHAP values for each prediction\n",
    "# Generate feature importance rankings\n",
    "# Create individual prediction explanations\n",
    "# Prepare text explanations for results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79476c28",
   "metadata": {},
   "source": [
    "## 10. Visualiza√ß√µes dos Resultados\n",
    "\n",
    "### 10.1 Matriz de visualiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "df = pd.DataFrame\n",
    "\n",
    "# Adicionar n√≥s (disciplinas)\n",
    "for sigla in df['DISCIPLINA'].unique():\n",
    "    G.add_node(sigla)\n",
    "\n",
    "# Modificar a cria√ß√£o de arestas\n",
    "for pair in similar_pairs:\n",
    "    disciplina_a, disciplina_b, similarity = pair\n",
    "    if similarity >= 0.8:  # Ajuste o threshold aqui\n",
    "        G.add_edge(disciplina_a, disciplina_b, weight=similarity)\n",
    "\n",
    "        # Calcular posi√ß√µes dos n√≥s\n",
    "        pos = nx.kamada_kawai_layout(G, weight='weight')  # Usa o peso (similaridade) para organizar\n",
    "\n",
    "        # Criar tra√ßos para arestas\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])  # None para separar linhas\n",
    "            edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line= dict(width=1, color='#888'),  # Espessura e cor das arestas\n",
    "            hoverinfo='none',\n",
    "            mode='lines')\n",
    "\n",
    "# Criar tra√ßos para n√≥s\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_text = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)  # Texto ao passar o mouse\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "  x=node_x, y=node_y,\n",
    "  mode='markers+text',\n",
    "  text=node_text,\n",
    "  textposition=\"top center\",\n",
    "  hoverinfo='text',\n",
    "  marker=dict(\n",
    "      showscale=True,\n",
    "      colorscale='YlGnBu',\n",
    "      size=15,\n",
    "      color=[],  # Pode ser usado para codificar cores por comunidade\n",
    "      line=dict(width=2, color='black'))\n",
    ")\n",
    "\n",
    "# Criar figura\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "    layout=go.Layout(\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=0, l=0, r=0, t=0),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    ")\n",
    "\n",
    "# Adicionar interatividade (exibir sigla ao passar o mouse)\n",
    "fig.update_traces(textposition='top center', hoverinfo='text')\n",
    "\n",
    "from networkx.algorithms import community\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "# Atribuir cores diferentes a cada comunidade\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create DiGraph visualization showing prerequisite relationships\n",
    "# Color nodes based on equivalence status\n",
    "# Highlight connected discipline pairs\n",
    "# Add node labels and edge weights\n",
    "# Save graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d967",
   "metadata": {},
   "source": [
    "### 10.2 Visualiza√ß√£o por mapa de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79b1aed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'similaridades_disciplinas_filtrado.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Carrega o TSV\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33msimilaridades_disciplinas_filtrado.tsv\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Seleciona 50 pares representativos\u001b[39;00m\n\u001b[32m      9\u001b[39m amostras = pd.concat([\n\u001b[32m     10\u001b[39m     df.nlargest(\u001b[32m10\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscore_combinado\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     11\u001b[39m     df.nsmallest(\u001b[32m10\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscore_combinado\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     df.sample(\u001b[32m10\u001b[39m, random_state=\u001b[32m7\u001b[39m)\n\u001b[32m     15\u001b[39m ]).drop_duplicates().reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = TextFileReader(filepath_or_buffer, **kwds)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28mself\u001b[39m._make_engine(f, \u001b[38;5;28mself\u001b[39m.engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = get_handle(\n\u001b[32m   1881\u001b[39m     f,\n\u001b[32m   1882\u001b[39m     mode,\n\u001b[32m   1883\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1884\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1885\u001b[39m     memory_map=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mmemory_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1886\u001b[39m     is_text=is_text,\n\u001b[32m   1887\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mencoding_errors\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1888\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1889\u001b[39m )\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artem\\miniconda3\\envs\\node2vec_env\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'similaridades_disciplinas_filtrado.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carrega o TSV\n",
    "df = pd.read_csv(\"similaridades_disciplinas_filtrado.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Seleciona 50 pares representativos\n",
    "amostras = pd.concat([\n",
    "    df.nlargest(10, 'score_combinado'),\n",
    "    df.nsmallest(10, 'score_combinado'),\n",
    "    df.nlargest(10, 'node2vec'),\n",
    "    df[df['jaccard_pred'] > 0].sample(10, random_state=42),\n",
    "    df.sample(10, random_state=7)\n",
    "]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Indexa√ß√£o leg√≠vel\n",
    "amostras.index = amostras[\"disciplina_a\"] + \" vs \" + amostras[\"disciplina_b\"]\n",
    "\n",
    "# M√©tricas para o heatmap\n",
    "metricas = [\"score_combinado\", \"jaccard_pred\", \"jaccard_succ\", \"profundidade\", \"node2vec\"]\n",
    "\n",
    "# Cria o heatmap\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(amostras[metricas], annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "plt.title(\"Mapa de Calor: Similaridade Entre Disciplinas (50 pares)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd844e83",
   "metadata": {},
   "source": [
    "O gr√°fico abaixo apresenta um heatmap com 50 pares de disciplinas selecionados de forma representativa para demonstrar padr√µes nas m√©tricas de similaridade utilizadas:\n",
    "\n",
    "score_combinado: medida geral de similaridade calculada com base em m√∫ltiplos crit√©rios.\n",
    "\n",
    "jaccard_pred e jaccard_succ: medem sobreposi√ß√£o de pr√©-requisitos e sucessores, respectivamente.\n",
    "\n",
    "profundidade: indica o qu√£o pr√≥ximas as disciplinas est√£o em termos de n√≠vel curricular.\n",
    "\n",
    "node2vec: representa a semelhan√ßa estrutural entre disciplinas no grafo de curr√≠culo.\n",
    "\n",
    "Os pares foram escolhidos para cobrir casos com alta e baixa similaridade, fortes conex√µes por embeddings, e exemplos aleat√≥rios, oferecendo interpret√°vel das rela√ß√µes entre disciplinas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce9bc0",
   "metadata": {},
   "source": [
    "![alt text](mapac-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcaaf6",
   "metadata": {},
   "source": [
    "### 10.3 Matrizes de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbad8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for model performance\n",
    "# Create heatmap visualization of confusion matrix\n",
    "# Add precision, recall, and F1 scores\n",
    "# Generate performance metrics report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868a7f5",
   "metadata": {},
   "source": [
    "## 11. Exporta√ß√£o de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f65c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered results to TSV file\n",
    "# Export model performance metrics\n",
    "# Save visualizations in specified formats\n",
    "# Generate final summary report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "node2vec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
